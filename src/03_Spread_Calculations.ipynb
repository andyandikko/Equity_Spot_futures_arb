{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfbbcbc-38ad-437b-810e-347be6842bee",
   "metadata": {},
   "source": [
    "# Equity Spot-Futures Arbitrage: Implied Forward Rate Computation\n",
    "\n",
    "This notebook demonstrates how to compute implied forward rates from equity index futures, mirroring the approach of Hazelkorn et al. (2021). Our goal is to construct:\n",
    "\n",
    "$$\n",
    "\\text{ESF}_t \\;=\\; f_{t,\\tau_1,\\tau_2} \\;-\\; \\text{OIS3M}_t\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f_{t,\\tau_1,\\tau_2} \\;=\\; \\frac{F_{t,\\tau_2} + \\mathbb{E}_t^Q[D_{t,\\tau_2}]}{F_{t,\\tau_1} + \\mathbb{E}_t^Q[D_{t,\\tau_1}]} - 1,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "F_{t,\\tau} \\;=\\; S_t\\bigl(1 + r_{t,\\tau}^f\\bigr) \\;-\\; \\mathbb{E}_t^Q[D_{t,\\tau}].\n",
    "$$\n",
    "\n",
    "The difference between the **futures-implied forward rate** and the **3-month OIS rate** can reveal potential funding frictions or other market constraints that prevent perfect spot-futures arbitrage in equity markets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f9ae1-77a8-4bc0-bd6d-2c1823cf4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the src directory to the path to import our settings\n",
    "sys.path.insert(1, \"./src\")\n",
    "try:\n",
    "    from settings import config\n",
    "    print(\"Successfully imported config from settings module\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import config. Using local fallback.\")\n",
    "    def config(key):\n",
    "        config_dict = {\n",
    "            \"DATA_DIR\": Path(\"./_data\"),\n",
    "            \"TEMP_DIR\": Path(\"./_data/temp\"),\n",
    "            \"INPUT_DIR\": Path(\"./_data/input\"),\n",
    "            \"PROCESSED_DIR\": Path(\"./_data/processed\"),\n",
    "            \"MANUAL_DATA_DIR\": Path(\"./data_manual\"),\n",
    "            \"OUTPUT_DIR\": Path(\"./_output\"),\n",
    "            \"START_DATE\": \"2010-01-01\",\n",
    "            \"END_DATE\": \"2025-01-01\"\n",
    "        }\n",
    "        return config_dict.get(key, Path(\"./data\"))\n",
    "\n",
    "# Basic paths\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "TEMP_DIR = config(\"TEMP_DIR\")\n",
    "INPUT_DIR = config(\"INPUT_DIR\")\n",
    "PROCESSED_DIR = config(\"PROCESSED_DIR\")\n",
    "DATA_MANUAL = config(\"MANUAL_DATA_DIR\")\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "\n",
    "START_DATE = pd.to_datetime(config(\"START_DATE\"))\n",
    "END_DATE = pd.to_datetime(config(\"END_DATE\"))\n",
    "\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"PROCESSED_DIR = {PROCESSED_DIR}\")\n",
    "print(f\"START_DATE = {START_DATE.date()}, END_DATE = {END_DATE.date()}\")\n",
    "\n",
    "INDEX_CODES = [\"SPX\", \"NDX\", \"INDU\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91337d20-5f64-4548-828c-97624f333111",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "In classical spot-futures parity for an equity index \\(S_t\\) that pays dividends from \\(t\\) to \\(t + \\tau\\), the no-arbitrage futures price is:\n",
    "\n",
    "$$\n",
    "F_{t,\\tau} \\;=\\; S_t\\,(1 + r_{t,\\tau}^f) \\;-\\; \\mathbb{E}_t^Q[D_{t,\\tau}].\n",
    "$$\n",
    "\n",
    "However, a direct spot vs. futures comparison can be noisy if they close at different times (e.g., 4:00 pm vs. 4:15 pm). Instead, we look at two futures contracts (maturing at \\(\\tau_1\\) and \\(\\tau_2\\)):\n",
    "\n",
    "$$\n",
    "1 + f_{t,\\tau_1,\\tau_2}\n",
    "\\;=\\;\n",
    "\\frac{F_{t,\\tau_2} + \\mathbb{E}_t^Q[D_{t,\\tau_2}]}{F_{t,\\tau_1} + \\mathbb{E}_t^Q[D_{t,\\tau_1}]},\n",
    "$$\n",
    "\n",
    "to isolate the **implied forward rate** \\(f_{t,\\tau_1,\\tau_2}\\). We then define the **Equity Spot-Futures Arbitrage Spread** as:\n",
    "\n",
    "$$\n",
    "\\text{ESF}_t \\;=\\; f_{t,\\tau_1,\\tau_2} \\;-\\; \\text{OIS3M}_t.\n",
    "$$\n",
    "\n",
    "A positive \\(\\text{ESF}_t\\) indicates the implied equity forward rate is higher than the OIS benchmark, potentially signaling a funding-cost wedge or other limits to arbitrage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de298e3-63b9-44a7-a000-0c086c9006ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily_dividends(index_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load daily dividends for the given index code from bloomberg_historical_data.parquet.\n",
    "    Return columns: [Date, Daily_Div].\n",
    "    \"\"\"\n",
    "    print(f\"[{index_code}] Building daily dividend table\")\n",
    "\n",
    "    input_file = Path(INPUT_DIR) / \"bloomberg_historical_data.parquet\"\n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"Primary input file not found, switching to cached data\")\n",
    "        input_file = Path(DATA_MANUAL) / \"bloomberg_historical_data.parquet\"\n",
    "\n",
    "    raw_df = pd.read_parquet(input_file)\n",
    "    div_col = (f\"{index_code} Index\", \"INDX_GROSS_DAILY_DIV\")\n",
    "    if div_col not in raw_df.columns:\n",
    "        raise ValueError(f\"Missing daily dividend column {div_col} for index={index_code}\")\n",
    "\n",
    "    div_df = raw_df.loc[:, div_col].to_frame(\"Daily_Div\").reset_index()\n",
    "    div_df.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
    "    div_df[\"Date\"] = pd.to_datetime(div_df[\"Date\"], errors=\"coerce\")\n",
    "    div_df[\"Daily_Div\"] = div_df[\"Daily_Div\"].fillna(0)\n",
    "\n",
    "    # Optionally drop any row that has no valid date\n",
    "    before_drop = len(div_df)\n",
    "    div_df.dropna(subset=[\"Date\"], inplace=True)\n",
    "    after_drop = len(div_df)\n",
    "    if after_drop < before_drop:\n",
    "        print(f\"[{index_code}] Dropped {before_drop - after_drop} rows with invalid or missing date in daily_div.\")\n",
    "\n",
    "    div_df.sort_values(\"Date\", inplace=True)\n",
    "    div_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"[{index_code}] daily dividends final shape: {div_df.shape}\")\n",
    "    print(f\"[{index_code}] Sample daily dividends:\\n{div_df.head(10)}\")\n",
    "    return div_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b54949-b2f2-4652-b838-6b3895a1e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barndorff_nielsen_filter(df: pd.DataFrame,\n",
    "                             colname: str,\n",
    "                             date_col: str = \"Date\",\n",
    "                             window: int = 45,\n",
    "                             threshold: float = 10.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Barndorff-Nielsen outlier filter on 'colname' over ±window days.\n",
    "    1) rolling median => ...\n",
    "    2) abs_dev from that median\n",
    "    3) rolling mean(abs_dev) => mad\n",
    "    4) outlier if abs_dev/mad >= threshold => set colname_filtered=NaN\n",
    "    \"\"\"\n",
    "    df = df.sort_values(date_col).copy()\n",
    "\n",
    "    rolling_median = df[colname].rolling(window=window*2+1, center=True, min_periods=1).median()\n",
    "    rolling_median_shifted = rolling_median.shift(1)\n",
    "\n",
    "    df[\"abs_dev\"] = (df[colname] - rolling_median_shifted).abs()\n",
    "    rolling_mad = df[\"abs_dev\"].rolling(window=window*2+1, center=True, min_periods=1).mean()\n",
    "    rolling_mad_shifted = rolling_mad.shift(1)\n",
    "\n",
    "    df[\"bad_price\"] = (df[\"abs_dev\"] / rolling_mad_shifted) >= threshold\n",
    "    df.loc[df[colname].isna(), \"bad_price\"] = False\n",
    "\n",
    "    # Count how many outliers\n",
    "    outlier_count = df[\"bad_price\"].sum()\n",
    "    if outlier_count > 0:\n",
    "        print(f\"Barndorff-Nielsen filter: flagged {int(outlier_count)} outliers in {colname}\")\n",
    "\n",
    "    df[f\"{colname}_filtered\"] = df[colname].where(~df[\"bad_price\"], np.nan)\n",
    "\n",
    "    df.drop([\"abs_dev\", \"bad_price\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c9128-dfb7-42b1-80b3-463e779000bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_index_forward_rates(index_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Load near/next futures for index_code from _Calendar_spread.csv\n",
    "    2) Merge with single OIS_3M (as-of)\n",
    "    3) Merge daily dividends, compute Div_Sum1_Comp & Div_Sum2_Comp\n",
    "    4) Implied forward => cal_{index_code}_rf, OIS forward => ois_fwd_{index_code}, spread\n",
    "    5) Barndorff outlier filter, then multiply spread by 100 => bps\n",
    "    6) Return the DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"[{index_code}] Starting forward rate computation\")\n",
    "\n",
    "    fut_file = Path(PROCESSED_DIR) / f\"{index_code}_Calendar_spread.csv\"\n",
    "    if not fut_file.exists():\n",
    "        print(f\"[{index_code}] Missing futures file: {fut_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    fut_df = pd.read_csv(fut_file)\n",
    "    print(f\"[{index_code}] Loaded futures shape: {fut_df.shape}\")\n",
    "\n",
    "    if \"Date\" not in fut_df.columns:\n",
    "        print(f\"[{index_code}] No 'Date' column in {fut_file}, aborting.\")\n",
    "        return pd.DataFrame()\n",
    "    fut_df[\"Date\"] = pd.to_datetime(fut_df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    before_drop = len(fut_df)\n",
    "    fut_df.dropna(subset=[\"Date\"], inplace=True)\n",
    "    print(f\"[{index_code}] Dropped {before_drop - len(fut_df)} rows lacking a valid Date in futures.\")\n",
    "    fut_df[\"Term1_SettlementDate\"] = pd.to_datetime(fut_df[\"Term1_SettlementDate\"], errors=\"coerce\")\n",
    "    fut_df[\"Term2_SettlementDate\"] = pd.to_datetime(fut_df[\"Term2_SettlementDate\"], errors=\"coerce\")\n",
    "\n",
    "    fut_df.sort_values(\"Date\", inplace=True)\n",
    "    fut_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # === Merge single OIS_3M\n",
    "    ois_file = Path(PROCESSED_DIR) / \"cleaned_ois_rates.csv\"\n",
    "    if not ois_file.exists():\n",
    "        print(f\"[{index_code}] Missing OIS file: {ois_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ois_df = pd.read_csv(ois_file)\n",
    "    if \"Date\" not in ois_df.columns:\n",
    "        ois_df.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    ois_df[\"Date\"] = pd.to_datetime(ois_df[\"Date\"], errors=\"coerce\")\n",
    "    ois_df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    # as-of merge\n",
    "    prev_len = len(fut_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        fut_df, ois_df, on=\"Date\", direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    print(f\"[{index_code}] as-of merged OIS: from {prev_len} -> {after_len} rows (should be same).\")\n",
    "\n",
    "    # rename OIS_3M => 'OIS'\n",
    "    if \"OIS_3M\" in merged_df.columns:\n",
    "        merged_df.rename(columns={\"OIS_3M\": \"OIS\"}, inplace=True)\n",
    "    else:\n",
    "        print(f\"[{index_code}] 'OIS_3M' column not found in OIS data, using default 'OIS_3M'?\")\n",
    "\n",
    "    # === Load daily dividends\n",
    "    div_df = build_daily_dividends(index_code)\n",
    "    # add cumsum in div_df\n",
    "    div_df[\"CumDiv\"] = div_df[\"Daily_Div\"].cumsum()\n",
    "\n",
    "    # merge cumsum at current date\n",
    "    prev_len = len(merged_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Date\"),\n",
    "        div_df[[\"Date\", \"CumDiv\"]].sort_values(\"Date\"),\n",
    "        on=\"Date\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    print(\n",
    "        f\"[{index_code}] as-of merged CumDiv at current date: from {prev_len} -> {after_len} rows.\"\n",
    "    )\n",
    "    merged_df.rename(columns={\"CumDiv\": \"CumDiv_current\"}, inplace=True)\n",
    "    print(f\"[{index_code}] Sample merged rows with cumulative div:\\n{merged_df.head(10)}\")\n",
    "    \n",
    "    # same approach for Term1 & Term2\n",
    "    t1_df = div_df.rename(columns={\"Date\": \"Term1_SettlementDate\", \"CumDiv\": \"CumDiv_Term1\"})\n",
    "    prev_len = len(merged_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Term1_SettlementDate\"),\n",
    "        t1_df.sort_values(\"Term1_SettlementDate\"),\n",
    "        on=\"Term1_SettlementDate\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    print(\n",
    "        f\"[{index_code}] as-of merged CumDiv for Term1: from {prev_len} -> {after_len} rows.\"\n",
    "    )\n",
    "\n",
    "    t2_df = div_df.rename(columns={\"Date\": \"Term2_SettlementDate\", \"CumDiv\": \"CumDiv_Term2\"})\n",
    "    prev_len = len(merged_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Term2_SettlementDate\"),\n",
    "        t2_df.sort_values(\"Term2_SettlementDate\"),\n",
    "        on=\"Term2_SettlementDate\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    print(\n",
    "        f\"[{index_code}] as-of merged CumDiv for Term2: from {prev_len} -> {after_len} rows.\"\n",
    "    )\n",
    "\n",
    "    # compute Div_Sum1 & Div_Sum2\n",
    "    merged_df[\"Div_Sum1\"] = merged_df[\"CumDiv_Term1\"] - merged_df[\"CumDiv_current\"]\n",
    "    merged_df[\"Div_Sum2\"] = merged_df[\"CumDiv_Term2\"] - merged_df[\"CumDiv_current\"]\n",
    "\n",
    "    # Handle missing TTM or price\n",
    "    # If TTM is missing, can't compute rates => drop\n",
    "    before_drop = len(merged_df)\n",
    "    merged_df.dropna(subset=[\"Term1_TTM\", \"Term2_TTM\", \"Term1_Futures_Price\", \"Term2_Futures_Price\"], inplace=True)\n",
    "    print(\n",
    "        f\"[{index_code}] Dropped {before_drop - len(merged_df)} rows missing TTM or Futures_Price.\"\n",
    "    )\n",
    "\n",
    "    # 4) Compounding\n",
    "    ttm1 = \"Term1_TTM\"\n",
    "    ttm2 = \"Term2_TTM\"\n",
    "    merged_df[\"Div_Sum1_Comp\"] = merged_df[\"Div_Sum1\"] * (\n",
    "        ((merged_df[ttm1] / 2.0) / 360.0) * merged_df[\"OIS\"] + 1.0\n",
    "    )\n",
    "    merged_df[\"Div_Sum2_Comp\"] = merged_df[\"Div_Sum2\"] * (\n",
    "        ((merged_df[ttm2] / 2.0) / 360.0) * merged_df[\"OIS\"] + 1.0\n",
    "    )\n",
    "\n",
    "    # Implied Forward\n",
    "    fp1 = \"Term1_Futures_Price\"\n",
    "    fp2 = \"Term2_Futures_Price\"\n",
    "    merged_df[\"implied_forward_raw\"] = (\n",
    "        (merged_df[fp2] + merged_df[\"Div_Sum2_Comp\"]) /\n",
    "        (merged_df[fp1] + merged_df[\"Div_Sum1_Comp\"])\n",
    "        - 1.0\n",
    "    )\n",
    "\n",
    "    dt = merged_df[ttm2] - merged_df[ttm1]\n",
    "    merged_df[f\"cal_{index_code}_rf\"] = np.where(\n",
    "        dt > 0,\n",
    "        100.0 * merged_df[\"implied_forward_raw\"] * (360.0 / dt),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # OIS-implied forward\n",
    "    merged_df[\"ois_fwd_raw\"] = (\n",
    "        (1.0 + merged_df[\"OIS\"] * merged_df[ttm2] / 360.0) /\n",
    "        (1.0 + merged_df[\"OIS\"] * merged_df[ttm1] / 360.0)\n",
    "        - 1.0\n",
    "    )\n",
    "    merged_df[f\"ois_fwd_{index_code}\"] = np.where(\n",
    "        dt > 0,\n",
    "        merged_df[\"ois_fwd_raw\"] * (360.0 / dt) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Spread\n",
    "    spread_col = f\"spread_{index_code}\"\n",
    "    merged_df[spread_col] = merged_df[f\"cal_{index_code}_rf\"] - merged_df[f\"ois_fwd_{index_code}\"]\n",
    "    \n",
    "    # BN outlier filter\n",
    "    merged_df = barndorff_nielsen_filter(merged_df, spread_col, date_col=\"Date\", window=45, threshold=10)\n",
    "    \n",
    "    # If outlier => set cal_rf & spread to NaN\n",
    "    out_mask = merged_df[f\"{spread_col}_filtered\"].isna()\n",
    "    outliers_count = out_mask.sum()\n",
    "    if outliers_count > 0:\n",
    "        print(f\"[{index_code}] Setting {outliers_count} outliers to NaN for cal_{index_code}_rf & {spread_col}\")\n",
    "    merged_df.loc[out_mask, f\"cal_{index_code}_rf\"] = np.nan\n",
    "    merged_df.loc[out_mask, spread_col] = np.nan\n",
    "\n",
    "    # Multiply spread by 100 => bps\n",
    "    merged_df[spread_col] = merged_df[spread_col] * 100.0\n",
    "    merged_df.set_index(\"Date\", inplace=True)\n",
    "    \n",
    "    print(f\"[{index_code}] Final forward rates shape: {merged_df.shape}\")\n",
    "    print(\n",
    "        f\"[{index_code}] Sample final rows:\\n\"\n",
    "        + merged_df[[f\"cal_{index_code}_rf\", f\"ois_fwd_{index_code}\", spread_col]].tail(5).to_string()\n",
    "    )\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e780e-b8da-49e3-b5be-34f395103030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_indices(results: dict, keep_dates: bool = True):\n",
    "    \"\"\"\n",
    "    Generate two charts for each index's final spread_{idx} in basis points:\n",
    "    1. From START_DATE to 2020-01-01\n",
    "    2. From START_DATE to END_DATE (full range)\n",
    "    \n",
    "    By default (keep_dates=True), we reindex to the union of all dates to keep the\n",
    "    X-axis from skipping days that are missing in some index.\n",
    "    \n",
    "    If you want to only show existing dates in each index's data, set keep_dates=False.\n",
    "    \"\"\"\n",
    "    # Define date ranges\n",
    "    pre_2020_end = pd.to_datetime('2020-01-01')\n",
    "    \n",
    "    # If keep_dates: find the union of all dates across all DataFrames\n",
    "    if keep_dates:\n",
    "        all_dates = set()\n",
    "        for idx, df in results.items():\n",
    "            if df is not None and not df.empty:\n",
    "                all_dates.update(df.index.tolist())\n",
    "        # Build a sorted list\n",
    "        date_index = pd.to_datetime(sorted(all_dates))\n",
    "    else:\n",
    "        date_index = None\n",
    "\n",
    "    colors = {\"SPX\": \"blue\", \"NDX\": \"green\", \"INDU\": \"red\"}\n",
    "    \n",
    "    # Create Figure 1: START_DATE to 2020-01-01\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, df in results.items():\n",
    "        if df is not None and not df.empty:\n",
    "            spread_col = f\"spread_{idx}\"\n",
    "            # Filter to pre-2020 data\n",
    "            pre_2020_df = df[df.index <= pre_2020_end]\n",
    "            \n",
    "            # If empty after filtering, skip\n",
    "            if pre_2020_df.empty:\n",
    "                continue\n",
    "                \n",
    "            # reindex if desired\n",
    "            if keep_dates and date_index is not None:\n",
    "                filtered_dates = date_index[date_index <= pre_2020_end]\n",
    "                df_plot = pre_2020_df.reindex(filtered_dates).ffill()  # Forward fill missing values\n",
    "            else:\n",
    "                df_plot = pre_2020_df\n",
    "                \n",
    "            plt.plot(\n",
    "                df_plot.index,\n",
    "                df_plot[spread_col],\n",
    "                color=colors.get(idx, \"black\"),\n",
    "                alpha=0.8,\n",
    "                label=f\"{idx} Spread (bps)\"\n",
    "            )\n",
    "\n",
    "    plt.axhline(0, color=\"k\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(\"Implied Forward Spread Across Indices (Pre-2020, bps)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Spread (bps)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    \n",
    "    # Display the first plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Create Figure 2: START_DATE to END_DATE (full range)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for idx, df in results.items():\n",
    "        if df is not None and not df.empty:\n",
    "            spread_col = f\"spread_{idx}\"\n",
    "            # reindex if desired\n",
    "            if keep_dates and date_index is not None:\n",
    "                df_plot = df.reindex(date_index).ffill()  # Forward fill missing values\n",
    "            else:\n",
    "                df_plot = df\n",
    "                \n",
    "            plt.plot(\n",
    "                df_plot.index,\n",
    "                df_plot[spread_col],\n",
    "                color=colors.get(idx, \"black\"),\n",
    "                alpha=0.8,\n",
    "                label=f\"{idx} Spread (bps)\"\n",
    "            )\n",
    "\n",
    "    plt.axhline(0, color=\"k\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(\"Implied Forward Spread Across Indices (Full Range, bps)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Spread (bps)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(plt.matplotlib.dates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    \n",
    "    # Display the second plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28572468-b10c-4f77-8c48-e4f07114468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"== Starting forward rate calculations with compounding dividends, single OIS, BN outlier filter ==\")\n",
    "    results = {}\n",
    "    for idx in INDEX_CODES:\n",
    "        df_res = process_index_forward_rates(idx)\n",
    "        results[idx] = df_res\n",
    "    \n",
    "    # Create both plots\n",
    "    plot_all_indices(results, keep_dates=True)\n",
    "\n",
    "    print(\"All computations completed successfully.\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0355d-a7d7-41ec-9328-5f3a185e2f3c",
   "metadata": {},
   "source": [
    "# Exploring the Functions: Visual Understanding of Implied Forward Rate Calculation\n",
    "\n",
    "This notebook provides a step-by-step exploration of each function in the implied forward rate calculation process, with visualizations and tables to help build intuition about what's happening at each stage.\n",
    "\n",
    "1. Data loading and preprocessing\n",
    "2. Daily dividend accumulation\n",
    "3. Barndorff-Nielsen outlier filtering mechanism\n",
    "4. The forward rate calculation process\n",
    "5. Visualization of results\n",
    "\n",
    "Let's break down each component visually to understand the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd427b04-7f7d-4055-a663-508457c315ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize and explain daily dividends\n",
    "def explore_daily_dividends(index_code=\"SPX\"):\n",
    "    \"\"\"\n",
    "    Explore and visualize how the daily dividend function works\n",
    "    \"\"\"\n",
    "    print(f\"=== Exploring build_daily_dividends for {index_code} ===\")\n",
    "    \n",
    "    # Load the data\n",
    "    input_file = Path(INPUT_DIR) / \"bloomberg_historical_data.parquet\"\n",
    "    if not input_file.exists():\n",
    "        input_file = Path(DATA_MANUAL) / \"bloomberg_historical_data.parquet\"\n",
    "        \n",
    "    if not input_file.exists():\n",
    "        print(\"No Bloomberg data file found. Cannot proceed with exploration.\")\n",
    "        return None\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_df = pd.read_parquet(input_file)\n",
    "    \n",
    "    # Show the structure of the raw dataframe\n",
    "    print(\"\\nRaw Bloomberg data structure:\")\n",
    "    print(f\"Shape: {raw_df.shape}\")\n",
    "    print(\"Column structure (MultiIndex):\")\n",
    "    print(raw_df.columns[:10])  # Show first 10 columns\n",
    "    \n",
    "    # Extract dividend column\n",
    "    div_col = (f\"{index_code} Index\", \"INDX_GROSS_DAILY_DIV\")\n",
    "    if div_col not in raw_df.columns:\n",
    "        print(f\"Missing dividend column for {index_code}. Trying a different approach...\")\n",
    "        # Try to find any dividend-related columns\n",
    "        div_cols = [col for col in raw_df.columns if 'DIV' in str(col)]\n",
    "        if div_cols:\n",
    "            print(f\"Found potential dividend columns: {div_cols}\")\n",
    "            div_col = div_cols[0]\n",
    "        else:\n",
    "            print(\"No dividend columns found in the data.\")\n",
    "            return None\n",
    "    \n",
    "    # Process dividends as in the original function\n",
    "    div_df = raw_df.loc[:, div_col].to_frame(\"Daily_Div\").reset_index()\n",
    "    div_df.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
    "    div_df[\"Date\"] = pd.to_datetime(div_df[\"Date\"], errors=\"coerce\")\n",
    "    div_df[\"Daily_Div\"] = div_df[\"Daily_Div\"].fillna(0)\n",
    "    div_df.dropna(subset=[\"Date\"], inplace=True)\n",
    "    div_df.sort_values(\"Date\", inplace=True)\n",
    "    div_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Create cumulative sum column\n",
    "    div_df[\"CumDiv\"] = div_df[\"Daily_Div\"].cumsum()\n",
    "    \n",
    "    # Display processed data\n",
    "    print(\"\\nProcessed dividend data (first 5 rows):\")\n",
    "    display(div_df.head())\n",
    "    \n",
    "    # Visualize daily dividends\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Subplot 1: Daily dividends over time\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(div_df[\"Date\"], div_df[\"Daily_Div\"], 'b-', alpha=0.7, linewidth=1)\n",
    "    plt.title(f\"{index_code} Daily Dividends\")\n",
    "    plt.ylabel(\"Daily Div Amount\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Cumulative dividends\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(div_df[\"Date\"], div_df[\"CumDiv\"], 'g-', linewidth=2)\n",
    "    plt.title(f\"{index_code} Cumulative Dividends\")\n",
    "    plt.ylabel(\"Cumulative Div\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nDividend Data Summary:\")\n",
    "    summary = div_df[\"Daily_Div\"].describe()\n",
    "    display(summary)\n",
    "    \n",
    "    # Show time periods with zero and non-zero dividends\n",
    "    zero_div = div_df[div_df[\"Daily_Div\"] == 0].shape[0]\n",
    "    total = div_df.shape[0]\n",
    "    print(f\"Days with zero dividends: {zero_div} ({zero_div/total:.1%} of total)\")\n",
    "    print(f\"Days with non-zero dividends: {total - zero_div} ({(total-zero_div)/total:.1%} of total)\")\n",
    "    \n",
    "    # Monthly dividend patterns (Useful to see if there are dividend seasons)\n",
    "    div_df['month'] = div_df['Date'].dt.month\n",
    "    div_df['year'] = div_df['Date'].dt.year\n",
    "    \n",
    "    monthly_pattern = div_df.groupby('month')['Daily_Div'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_pattern.plot(kind='bar', color='teal')\n",
    "    plt.title(f\"{index_code} Average Daily Dividend by Month\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Average Daily Dividend\")\n",
    "    plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.show()\n",
    "    \n",
    "    return div_df\n",
    "\n",
    "# Run the exploration\n",
    "spx_div_df = explore_daily_dividends(\"SPX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73098464-6887-45f8-938a-4978820bca6f",
   "metadata": {},
   "source": [
    "## Understanding Daily Dividends\n",
    "\n",
    "The `build_daily_dividends()` function extracts daily dividend information from the Bloomberg data and prepares it for use in the forward rate calculations.\n",
    "\n",
    "Key observations:\n",
    "1. The function extracts a specific column from a multi-index DataFrame\n",
    "2. It converts dates, fills missing values with zeros, and sorts by date\n",
    "3. Daily dividends are typically sparse - many days have zero dividends\n",
    "4. The cumulative dividend is critical for the forward rate calculation\n",
    "5. There may be seasonal patterns in dividend payments\n",
    "\n",
    "The visualizations show both the daily dividend amounts (which are typically spiky) and the cumulative dividends (which grow over time). The monthly pattern chart helps identify any seasonality in dividend payments.\n",
    "\n",
    "These dividend values will be used in the `Div_Sum1` and `Div_Sum2` calculations, which represent the expected dividends between the current date and the settlement dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea5f9f-2695-4018-9209-5a9107ba36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_barndorff_nielsen_filter():\n",
    "    \"\"\"\n",
    "    Demonstrate how the Barndorff-Nielsen outlier filter works with a synthetic dataset\n",
    "    \"\"\"\n",
    "    print(\"=== Exploring Barndorff-Nielsen Filter ===\")\n",
    "    \n",
    "    # Create a synthetic time series with some outliers\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start='2020-01-01', periods=200)\n",
    "    \n",
    "    # Base series with some seasonality and trend\n",
    "    base = np.linspace(0, 10, 200) + 2 * np.sin(np.linspace(0, 10, 200))\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 0.5, 200)\n",
    "    \n",
    "    # Add outliers\n",
    "    outliers_idx = [20, 50, 80, 120, 150, 190]\n",
    "    outliers = np.zeros(200)\n",
    "    for idx in outliers_idx:\n",
    "        outliers[idx] = np.random.choice([-1, 1]) * np.random.uniform(3, 5)\n",
    "    \n",
    "    # Combined series\n",
    "    values = base + noise + outliers\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'value': values\n",
    "    })\n",
    "    \n",
    "    # Visualize the original series\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Date'], df['value'], 'b-', alpha=0.7)\n",
    "    for idx in outliers_idx:\n",
    "        plt.scatter(df['Date'][idx], df['value'][idx], color='red', s=80)\n",
    "    plt.title('Synthetic Time Series with Outliers')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply the Barndorff-Nielsen filter\n",
    "    window = 10  # Smaller window for demonstration\n",
    "    threshold = 3.0  # Lower threshold to catch all our synthetic outliers\n",
    "    \n",
    "    # Sort the data\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Calculate rolling median\n",
    "    rolling_median = df['value'].rolling(window=window*2+1, center=True, min_periods=1).median()\n",
    "    rolling_median_shifted = rolling_median.shift(1)\n",
    "    \n",
    "    # Calculate absolute deviation\n",
    "    df['abs_dev'] = (df['value'] - rolling_median_shifted).abs()\n",
    "    \n",
    "    # Calculate rolling MAD (Mean Absolute Deviation)\n",
    "    rolling_mad = df['abs_dev'].rolling(window=window*2+1, center=True, min_periods=1).mean()\n",
    "    rolling_mad_shifted = rolling_mad.shift(1)\n",
    "    \n",
    "    # Calculate standardized deviation\n",
    "    df['std_dev'] = df['abs_dev'] / rolling_mad_shifted\n",
    "    \n",
    "    # Determine outliers\n",
    "    df['is_outlier'] = df['std_dev'] >= threshold\n",
    "    df.loc[df['value'].isna(), 'is_outlier'] = False\n",
    "    \n",
    "    # Create filtered series\n",
    "    df['value_filtered'] = df['value'].where(~df['is_outlier'], np.nan)\n",
    "    \n",
    "    # Show step-by-step calculations for better understanding\n",
    "    print(\"\\nStep-by-step filter process (showing sample rows):\")\n",
    "    display(df[['Date', 'value', 'abs_dev', 'std_dev', 'is_outlier', 'value_filtered']].iloc[15:25])\n",
    "    \n",
    "    # Visualize the filtering steps\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
    "    \n",
    "    # Original with rolling median\n",
    "    axes[0].plot(df['Date'], df['value'], 'b-', alpha=0.7, label='Original')\n",
    "    axes[0].plot(df['Date'], rolling_median, 'r-', alpha=0.7, label='Rolling Median')\n",
    "    for idx in outliers_idx:\n",
    "        axes[0].scatter(df['Date'][idx], df['value'][idx], color='red', s=80)\n",
    "    axes[0].set_title('Original Series with Rolling Median')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Absolute Deviation and Mean Absolute Deviation\n",
    "    axes[1].plot(df['Date'], df['abs_dev'], 'g-', alpha=0.7, label='Absolute Deviation')\n",
    "    axes[1].plot(df['Date'], rolling_mad, 'm-', alpha=0.7, label='Mean Absolute Deviation')\n",
    "    axes[1].set_title('Absolute Deviation and MAD')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Standardized Deviation with Threshold\n",
    "    axes[2].plot(df['Date'], df['std_dev'], 'c-', alpha=0.7, label='Standardized Deviation')\n",
    "    axes[2].axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n",
    "    for idx in outliers_idx:\n",
    "        if idx < len(df) and df['is_outlier'].iloc[idx]:\n",
    "            axes[2].scatter(df['Date'][idx], df['std_dev'][idx], color='red', s=80)\n",
    "    axes[2].set_title('Standardized Deviation with Outlier Threshold')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare original vs filtered\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Date'], df['value'], 'b-', alpha=0.5, label='Original')\n",
    "    plt.plot(df['Date'], df['value_filtered'], 'g-', linewidth=2, label='Filtered')\n",
    "    detected_outliers = df[df['is_outlier']].index\n",
    "    plt.scatter(df['Date'][detected_outliers], df['value'][detected_outliers], \n",
    "                color='red', s=80, label='Detected Outliers')\n",
    "    plt.title('Original vs Filtered Time Series')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary of outlier detection\n",
    "    detected_count = df['is_outlier'].sum()\n",
    "    print(f\"\\nSummary: Detected {detected_count} outliers out of {len(df)} observations\")\n",
    "    \n",
    "    # Check how many of our artificial outliers were detected\n",
    "    true_positive = 0\n",
    "    for idx in outliers_idx:\n",
    "        if idx < len(df) and df['is_outlier'].iloc[idx]:\n",
    "            true_positive += 1\n",
    "    \n",
    "    print(f\"True outliers detected: {true_positive} out of {len(outliers_idx)}\")\n",
    "    if len(outliers_idx) > 0:\n",
    "        print(f\"Detection rate: {true_positive/len(outliers_idx):.1%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the exploration\n",
    "bn_filter_df = explore_barndorff_nielsen_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac216c6-57e7-409e-8a34-8645c01a5154",
   "metadata": {},
   "source": [
    "## Understanding Barndorff-Nielsen Outlier Filter\n",
    "\n",
    "The Barndorff-Nielsen filter is a robust statistical method for detecting and removing outliers in time series data. Following the paper's methedology:\n",
    "\n",
    "1. **Rolling Median Calculation**: \n",
    "   - For each point, calculate the median value within a window (±45 days by default)\n",
    "   - This establishes a robust estimate of the \"normal\" value at each point\n",
    "\n",
    "2. **Absolute Deviation**:\n",
    "   - Calculate how far each value deviates from the rolling median\n",
    "   - The shift(1) ensures we're using previously known information\n",
    "\n",
    "3. **Mean Absolute Deviation (MAD)**:\n",
    "   - Calculate the average of absolute deviations within the window\n",
    "   - This establishes what constitutes a \"normal\" amount of deviation\n",
    "\n",
    "4. **Standardized Deviation**:\n",
    "   - Divide each absolute deviation by the MAD\n",
    "   - This creates a standardized measure of how unusual each deviation is\n",
    "\n",
    "5. **Outlier Detection**:\n",
    "   - Points where standardized deviation ≥ threshold (default 10.0) are marked as outliers\n",
    "   - These points are replaced with NaN in the filtered series\n",
    "\n",
    "The filter is particularly useful for financial time series where sudden jumps can occur due to data errors or extreme market events that shouldn't influence calculations of normal behavior.\n",
    "\n",
    "In the spread calculation, this prevents outliers from distorting the implied forward rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79696e3-83f6-48b4-8014-6ef9111b22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_forward_rate_calculation(index_code=\"SPX\"):\n",
    "    \"\"\"\n",
    "    Explore the steps of the forward rate calculation visually\n",
    "    \"\"\"\n",
    "    print(f\"=== Exploring Forward Rate Calculation for {index_code} ===\")\n",
    "    \n",
    "    # Load necessary data\n",
    "    fut_file = Path(PROCESSED_DIR) / f\"{index_code}_Calendar_spread.csv\"\n",
    "    ois_file = Path(PROCESSED_DIR) / \"cleaned_ois_rates.csv\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not fut_file.exists() or not ois_file.exists():\n",
    "        print(\"Required data files not found. Cannot proceed with exploration.\")\n",
    "        return None\n",
    "    \n",
    "    # Load calendar spreads\n",
    "    fut_df = pd.read_csv(fut_file)\n",
    "    fut_df[\"Date\"] = pd.to_datetime(fut_df[\"Date\"], errors=\"coerce\")\n",
    "    fut_df.dropna(subset=[\"Date\"], inplace=True)\n",
    "    fut_df[\"Term1_SettlementDate\"] = pd.to_datetime(fut_df[\"Term1_SettlementDate\"], errors=\"coerce\")\n",
    "    fut_df[\"Term2_SettlementDate\"] = pd.to_datetime(fut_df[\"Term2_SettlementDate\"], errors=\"coerce\")\n",
    "    fut_df.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    # Display calendar spread data\n",
    "    print(\"\\nFutures Calendar Spread data (first 5 rows):\")\n",
    "    display(fut_df.head())\n",
    "    \n",
    "    # Load OIS rates\n",
    "    ois_df = pd.read_csv(ois_file)\n",
    "    if \"Date\" not in ois_df.columns:\n",
    "        ois_df.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    ois_df[\"Date\"] = pd.to_datetime(ois_df[\"Date\"], errors=\"coerce\")\n",
    "    ois_df.sort_values(\"Date\", inplace=True)\n",
    "    \n",
    "    # Display OIS rates\n",
    "    print(\"\\nOIS rates data (first 5 rows):\")\n",
    "    display(ois_df.head())\n",
    "    \n",
    "    # Chart the OIS rates\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(ois_df[\"Date\"], ois_df[\"OIS_3M\"], 'g-', linewidth=2)\n",
    "    plt.title(\"3-Month OIS Rates Over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"OIS Rate (%)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Merge futures and OIS data\n",
    "    merged_df = pd.merge_asof(\n",
    "        fut_df.sort_values(\"Date\"),\n",
    "        ois_df,\n",
    "        on=\"Date\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    \n",
    "    if \"OIS_3M\" in merged_df.columns:\n",
    "        merged_df.rename(columns={\"OIS_3M\": \"OIS\"}, inplace=True)\n",
    "    \n",
    "    # Load dividends\n",
    "    div_df = build_daily_dividends(index_code)\n",
    "    div_df[\"CumDiv\"] = div_df[\"Daily_Div\"].cumsum()\n",
    "    \n",
    "    # Merge with current date cumulative dividends\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Date\"),\n",
    "        div_df[[\"Date\", \"CumDiv\"]].sort_values(\"Date\"),\n",
    "        on=\"Date\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    merged_df.rename(columns={\"CumDiv\": \"CumDiv_current\"}, inplace=True)\n",
    "    \n",
    "    # Merge with Term1 and Term2 settlement date cumulative dividends\n",
    "    t1_df = div_df.rename(columns={\"Date\": \"Term1_SettlementDate\", \"CumDiv\": \"CumDiv_Term1\"})\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Term1_SettlementDate\"),\n",
    "        t1_df.sort_values(\"Term1_SettlementDate\"),\n",
    "        on=\"Term1_SettlementDate\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    \n",
    "    t2_df = div_df.rename(columns={\"Date\": \"Term2_SettlementDate\", \"CumDiv\": \"CumDiv_Term2\"})\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Term2_SettlementDate\"),\n",
    "        t2_df.sort_values(\"Term2_SettlementDate\"),\n",
    "        on=\"Term2_SettlementDate\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    \n",
    "    # Calculate dividend sums\n",
    "    merged_df[\"Div_Sum1\"] = merged_df[\"CumDiv_Term1\"] - merged_df[\"CumDiv_current\"]\n",
    "    merged_df[\"Div_Sum2\"] = merged_df[\"CumDiv_Term2\"] - merged_df[\"CumDiv_current\"]\n",
    "    \n",
    "    # Display intermediate calculation\n",
    "    print(\"\\nIntermediate calculation with dividend sums (sample rows):\")\n",
    "    display(merged_df[[\"Date\", \"Term1_TTM\", \"Term2_TTM\", \"Term1_Futures_Price\", \n",
    "                      \"Term2_Futures_Price\", \"OIS\", \"Div_Sum1\", \"Div_Sum2\"]].head())\n",
    "    \n",
    "    # Remove rows with missing TTM or price data\n",
    "    merged_df.dropna(subset=[\"Term1_TTM\", \"Term2_TTM\", \"Term1_Futures_Price\", \"Term2_Futures_Price\"], inplace=True)\n",
    "    \n",
    "    # Step-by-step calculation visualization\n",
    "    \n",
    "    # 1. Compounding dividends\n",
    "    merged_df[\"Div_Sum1_Comp\"] = merged_df[\"Div_Sum1\"] * (\n",
    "        ((merged_df[\"Term1_TTM\"] / 2.0) / 360.0) * merged_df[\"OIS\"] + 1.0\n",
    "    )\n",
    "    merged_df[\"Div_Sum2_Comp\"] = merged_df[\"Div_Sum2\"] * (\n",
    "        ((merged_df[\"Term2_TTM\"] / 2.0) / 360.0) * merged_df[\"OIS\"] + 1.0\n",
    "    )\n",
    "    \n",
    "    # Show effect of compounding\n",
    "    print(\"\\nEffect of compounding on dividends (sample rows):\")\n",
    "    display(merged_df[[\"Date\", \"Div_Sum1\", \"Div_Sum1_Comp\", \"Div_Sum2\", \"Div_Sum2_Comp\"]].head())\n",
    "    \n",
    "    # 2. Implied forward rate calculation\n",
    "    merged_df[\"implied_forward_raw\"] = (\n",
    "        (merged_df[\"Term2_Futures_Price\"] + merged_df[\"Div_Sum2_Comp\"]) /\n",
    "        (merged_df[\"Term1_Futures_Price\"] + merged_df[\"Div_Sum1_Comp\"])\n",
    "        - 1.0\n",
    "    )\n",
    "    \n",
    "    dt = merged_df[\"Term2_TTM\"] - merged_df[\"Term1_TTM\"]\n",
    "    merged_df[f\"cal_{index_code}_rf\"] = np.where(\n",
    "        dt > 0,\n",
    "        100.0 * merged_df[\"implied_forward_raw\"] * (360.0 / dt),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # 3. OIS-implied forward rate\n",
    "    merged_df[\"ois_fwd_raw\"] = (\n",
    "        (1.0 + merged_df[\"OIS\"] * merged_df[\"Term2_TTM\"] / 360.0) /\n",
    "        (1.0 + merged_df[\"OIS\"] * merged_df[\"Term1_TTM\"] / 360.0)\n",
    "        - 1.0\n",
    "    )\n",
    "    merged_df[f\"ois_fwd_{index_code}\"] = np.where(\n",
    "        dt > 0,\n",
    "        merged_df[\"ois_fwd_raw\"] * (360.0 / dt) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # 4. Spread calculation\n",
    "    spread_col = f\"spread_{index_code}\"\n",
    "    merged_df[spread_col] = merged_df[f\"cal_{index_code}_rf\"] - merged_df[f\"ois_fwd_{index_code}\"]\n",
    "    \n",
    "    # Show forward rate calculations\n",
    "    print(\"\\nForward rate calculations (sample rows):\")\n",
    "    display(merged_df[[\n",
    "        \"Date\", \n",
    "        \"implied_forward_raw\", \n",
    "        f\"cal_{index_code}_rf\", \n",
    "        \"ois_fwd_raw\", \n",
    "        f\"ois_fwd_{index_code}\", \n",
    "        spread_col\n",
    "    ]].head())\n",
    "    \n",
    "    # Apply Barndorff-Nielsen filter\n",
    "    filtered_df = barndorff_nielsen_filter(merged_df, spread_col, date_col=\"Date\", window=45, threshold=10)\n",
    "    \n",
    "    # Set outliers to NaN\n",
    "    out_mask = filtered_df[f\"{spread_col}_filtered\"].isna()\n",
    "    filtered_df.loc[out_mask, f\"cal_{index_code}_rf\"] = np.nan\n",
    "    filtered_df.loc[out_mask, spread_col] = np.nan\n",
    "    \n",
    "    # Multiply spread by 100 for bps\n",
    "    filtered_df[spread_col] = filtered_df[spread_col] * 100.0\n",
    "    \n",
    "    # Set index to Date for time series plots\n",
    "    filtered_df.set_index(\"Date\", inplace=True)\n",
    "    \n",
    "    # Visualize the forward rates and spread\n",
    "    plt.figure(figsize=(12, 15))\n",
    "    \n",
    "    # Plot 1: Implied Forward Rate\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(filtered_df.index, filtered_df[f\"cal_{index_code}_rf\"], 'b-', linewidth=2)\n",
    "    plt.title(f\"{index_code} Implied Forward Rate\")\n",
    "    plt.ylabel(\"Rate (%)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: OIS Forward Rate\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(filtered_df.index, filtered_df[f\"ois_fwd_{index_code}\"], 'g-', linewidth=2)\n",
    "    plt.title(f\"{index_code} OIS-Implied Forward Rate\")\n",
    "    plt.ylabel(\"Rate (%)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Spread\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(filtered_df.index, filtered_df[spread_col], 'r-', linewidth=2)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.7)\n",
    "    plt.title(f\"{index_code} Forward Rate Spread (bps)\")\n",
    "    plt.ylabel(\"Spread (bps)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the term structure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get a sample date for demonstration\n",
    "    sample_date = filtered_df.index[len(filtered_df) // 2]  # Middle of the dataset\n",
    "    print(f\"\\nTerm structure visualization for date: {sample_date}\")\n",
    "    \n",
    "    # Filter for a reasonable window around the sample date\n",
    "    date_window = 5  # Days\n",
    "    sample_window = filtered_df.index.get_indexer([sample_date], method='nearest')[0]\n",
    "    sample_window_start = max(0, sample_window - date_window)\n",
    "    sample_window_end = min(len(filtered_df), sample_window + date_window)\n",
    "    \n",
    "    sample_data = filtered_df.iloc[sample_window_start:sample_window_end]\n",
    "    \n",
    "    # Plot term structure\n",
    "    for date in sample_data.index:\n",
    "        row = filtered_df.loc[date]\n",
    "        plt.scatter(row[\"Term1_TTM\"], row[f\"cal_{index_code}_rf\"], color='blue', label='Near Term' if date == sample_data.index[0] else \"\")\n",
    "        plt.scatter(row[\"Term2_TTM\"], row[f\"ois_fwd_{index_code}\"], color='green', label='Far Term' if date == sample_data.index[0] else \"\")\n",
    "        \n",
    "        # Connect the points with a line\n",
    "        plt.plot([row[\"Term1_TTM\"], row[\"Term2_TTM\"]], \n",
    "                 [row[f\"cal_{index_code}_rf\"], row[f\"ois_fwd_{index_code}\"]], \n",
    "                 'k--', alpha=0.3)\n",
    "    \n",
    "    plt.title(f\"{index_code} Forward Rate Term Structure\")\n",
    "    plt.xlabel(\"Time to Maturity (days)\")\n",
    "    plt.ylabel(\"Forward Rate (%)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Run the exploration\n",
    "forward_rate_df = explore_forward_rate_calculation(\"SPX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2b8bd-916d-4ff3-9279-6c44ac229cca",
   "metadata": {},
   "source": [
    "## Understanding the Forward Rate Calculation Process\n",
    "\n",
    "When computing **equity spot-futures arbitrage** for an index \\( S_t \\) paying dividends from \\( t \\) to \\( t + \\tau \\), we combine:\n",
    "\n",
    "- **Futures Prices** (from the nearest and next-nearest futures contracts)\n",
    "- **OIS Rates** (as a proxy for the risk-free rate)\n",
    "- **Realized Dividends** (as a simple forecast of expected dividends)\n",
    "\n",
    "These are merged into a single DataFrame, from which we derive an **implied forward rate** \\( \\cal{rf} \\) and compare it with the **OIS-implied forward rate** to compute an **arbitrage spread**.\n",
    "\n",
    "### 1. Data Preparation & Merging\n",
    "1. **Calendar Spread Data**  \n",
    "   Provides columns for:\n",
    "   - `Term1_Futures_Price` and `Term2_Futures_Price` (prices for near-term and deferred futures)\n",
    "   - `Term1_SettlementDate` and `Term2_SettlementDate`\n",
    "   - `Term1_TTM` and `Term2_TTM` (time-to-maturity in days)\n",
    "2. **OIS Rates**  \n",
    "   Contains daily 3-month OIS rates, merged into the calendar spread data via an as-of backward merge on `Date`.\n",
    "3. **Dividends**  \n",
    "   - We accumulate daily dividends into a `CumDiv` series.\n",
    "   - Merged at both the **current date** and each **futures settlement date** to approximate how much dividend would be paid between now and each maturity.\n",
    "\n",
    "### 2. Dividend Sums\n",
    "Let:\n",
    "- $ \\text{CumDiv}_\\text{current} $ = cumulative dividend at the current date\n",
    "- $ \\text{CumDiv}_{\\text{Term1}} $ = cumulative dividend at the Term1 settlement date\n",
    "- $ \\text{CumDiv}_{\\text{Term2}} $ = cumulative dividend at the Term2 settlement date\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Div\\_Sum1} = \\text{CumDiv}_{\\text{Term1}} - \\text{CumDiv}_\\text{current},\n",
    "\\quad\n",
    "\\text{Div\\_Sum2} = \\text{CumDiv}_{\\text{Term2}} - \\text{CumDiv}_\\text{current}.\n",
    "$$\n",
    "\n",
    "### 3. Dividend Compounding\n",
    "We apply a **half-interval compounding** for each dividend sum:\n",
    "\n",
    "$$\n",
    "\\text{Div\\_Sum1\\_Comp} \\;=\\; \\text{Div\\_Sum1} \\times\n",
    "\\Bigl(\\bigl(\\tfrac{\\text{Term1\\_TTM}}{2 \\times 360}\\bigr) \\times \\text{OIS} \\;+\\; 1.0\\Bigr).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Div\\_Sum2\\_Comp} \\;=\\; \\text{Div\\_Sum2} \\times\n",
    "\\Bigl(\\bigl(\\tfrac{\\text{Term2\\_TTM}}{2 \\times 360}\\bigr) \\times \\text{OIS} \\;+\\; 1.0\\Bigr).\n",
    "$$\n",
    "\n",
    "### 4. Implied Forward Rate\n",
    "From Hazelkorn et al. (2021), the **raw** implied forward rate (over \\(\\tau_1\\) to \\(\\tau_2\\)) is:\n",
    "\n",
    "$$\n",
    "\\text{implied\\_forward\\_raw} \\;=\\;\n",
    "\\frac{\\text{Term2\\_Futures\\_Price} + \\text{Div\\_Sum2\\_Comp}}{\\text{Term1\\_Futures\\_Price} + \\text{Div\\_Sum1\\_Comp}}\n",
    "\\;-\\; 1.\n",
    "$$\n",
    "\n",
    "We **annualize** this raw rate by multiplying according to the difference in time-to-maturity (\\(\\text{Term2\\_TTM} - \\text{Term1\\_TTM}\\)):\n",
    "\n",
    "$$\n",
    "\\text{cal\\_rf}_{index} \\;=\\;\n",
    "100 \\times \\text{implied\\_forward\\_raw} \\;\\times\\;\n",
    "\\frac{360}{\\text{Term2\\_TTM} - \\text{Term1\\_TTM}}.\n",
    "$$\n",
    "\n",
    "### 5. OIS-Implied Forward Rate\n",
    "Similarly, we estimate how **OIS** would imply a forward rate between \\(\\tau_1\\) and \\(\\tau_2\\):\n",
    "\n",
    "$$\n",
    "\\text{ois\\_fwd\\_raw} \\;=\\;\n",
    "\\frac{1 + \\text{OIS} \\times \\tfrac{\\text{Term2\\_TTM}}{360}}\n",
    "     {1 + \\text{OIS} \\times \\tfrac{\\text{Term1\\_TTM}}{360}}\n",
    "\\;-\\; 1.\n",
    "$$\n",
    "\n",
    "We annualize it in the same manner:\n",
    "\n",
    "$$\n",
    "\\text{ois\\_fwd}_{index} \\;=\\;\n",
    "\\text{ois\\_fwd\\_raw} \\;\\times\\;\n",
    "\\frac{360}{\\text{Term2\\_TTM} - \\text{Term1\\_TTM}} \\;\\times\\; 100.\n",
    "$$\n",
    "\n",
    "### 6. Spread Calculation\n",
    "We define the **arbitrage spread** in **basis points** (bps) as:\n",
    "\n",
    "$$\n",
    "\\text{spread}_{index} \\;=\\;\n",
    "\\bigl(\\text{cal\\_rf}_{index} - \\text{ois\\_fwd}_{index}\\bigr) \\times 100.\n",
    "$$\n",
    "\n",
    "A **positive** spread indicates the **futures-implied forward rate** is **higher** than the OIS-implied rate.\n",
    "\n",
    "### 7. Outlier Filtering\n",
    "We apply a **Barndorff–Nielsen filter** on `spread_{index}` to flag outliers based on rolling median absolute deviation. These flagged values are set to `NaN` to remove spurious spikes or data errors. \n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**  \n",
    "- **Spot-Futures Parity** would suggest the forward rate extracted from futures = OIS rate, making the spread near zero.  \n",
    "- A **non-zero** spread implies potential **frictions**, **funding constraints**, or **margin requirements** that might limit arbitrage.  \n",
    "\n",
    "1. **Futures + Dividends** yield an **equity-implied forward rate** over \\(\\tau_1 \\to \\tau_2\\).  \n",
    "2. **OIS** gives an alternative “risk-free” benchmark.  \n",
    "3. Comparing them highlights possible **arbitrage** or **funding market** dislocations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4283e-ac45-408f-92d7-127281dae5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comparing Forward Rates Across Indices\n",
    "\n",
    "def explore_cross_index_comparison():\n",
    "    \"\"\"\n",
    "    Compare forward rates and spreads across different indices to identify patterns and relationships\n",
    "    \"\"\"\n",
    "    print(\"=== Exploring Cross-Index Forward Rate Analysis ===\")\n",
    "    \n",
    "    # Process data for all indices\n",
    "    results = {}\n",
    "    for idx in INDEX_CODES:\n",
    "        print(f\"\\nProcessing {idx}...\")\n",
    "        \n",
    "        fut_file = Path(PROCESSED_DIR) / f\"{idx}_Calendar_spread.csv\"\n",
    "        if not fut_file.exists():\n",
    "            print(f\"  Missing futures file for {idx}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Run the forward rate calculation\n",
    "        df_res = process_index_forward_rates(idx)\n",
    "        if df_res is not None and not df_res.empty:\n",
    "            results[idx] = df_res\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid results to compare. Ending exploration.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a unified DataFrame with spreads from all indices\n",
    "    # First, find the union of all dates\n",
    "    all_dates = set()\n",
    "    for idx, df in results.items():\n",
    "        all_dates.update(df.index.tolist())\n",
    "    \n",
    "    # Sort dates\n",
    "    date_index = pd.to_datetime(sorted(all_dates))\n",
    "    \n",
    "    # Create a DataFrame with all spreads\n",
    "    spreads_df = pd.DataFrame(index=date_index)\n",
    "    \n",
    "    for idx, df in results.items():\n",
    "        spread_col = f\"spread_{idx}\"\n",
    "        # Forward fill to handle missing dates\n",
    "        spreads_df[spread_col] = df[spread_col].reindex(date_index).ffill()\n",
    "    \n",
    "    # Show the head of the combined spreads\n",
    "    print(\"\\nCombined spreads data (first rows):\")\n",
    "    display(spreads_df.head())\n",
    "    \n",
    "    # Visualize combined spreads\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = {\"SPX\": \"blue\", \"NDX\": \"green\", \"INDU\": \"red\"}\n",
    "    for idx in results.keys():\n",
    "        spread_col = f\"spread_{idx}\"\n",
    "        plt.plot(\n",
    "            spreads_df.index, \n",
    "            spreads_df[spread_col], \n",
    "            color=colors.get(idx, \"black\"),\n",
    "            alpha=0.8,\n",
    "            label=f\"{idx} Spread (bps)\"\n",
    "        )\n",
    "    \n",
    "    plt.axhline(0, color=\"k\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(\"Implied Forward Spreads Across Indices (bps)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Spread (bps)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlations between spreads\n",
    "    if len(results) > 1:\n",
    "        corr_matrix = spreads_df.corr()\n",
    "        print(\"\\nCorrelation matrix of spreads:\")\n",
    "        display(corr_matrix)\n",
    "        \n",
    "        # Visualize correlations\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, square=True)\n",
    "        plt.title(\"Correlation of Forward Rate Spreads Across Indices\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Z-score normalized spreads for comparison\n",
    "    if not spreads_df.empty:\n",
    "        # Calculate z-scores for each spread\n",
    "        z_scores_df = pd.DataFrame(index=spreads_df.index)\n",
    "        \n",
    "        for col in spreads_df.columns:\n",
    "            z_scores_df[col] = (spreads_df[col] - spreads_df[col].mean()) / spreads_df[col].std()\n",
    "        \n",
    "        # Visualize normalized spreads\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for idx in results.keys():\n",
    "            spread_col = f\"spread_{idx}\"\n",
    "            plt.plot(\n",
    "                z_scores_df.index, \n",
    "                z_scores_df[spread_col], \n",
    "                color=colors.get(idx, \"black\"),\n",
    "                alpha=0.8,\n",
    "                label=f\"{idx} (Z-score)\"\n",
    "            )\n",
    "        \n",
    "        plt.axhline(0, color=\"k\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.title(\"Z-Score Normalized Spreads Across Indices\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Z-Score\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    return spreads_df\n",
    "cross_index_df = explore_cross_index_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1626b7-d424-4a94-b9f6-9999b6450b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Exploring the Economic Significance of Forward Rate Spreads\n",
    "\n",
    "def explore_economic_significance():\n",
    "    \"\"\"\n",
    "    Analyze the economic significance of forward rate spreads\n",
    "    \"\"\"\n",
    "    print(\"=== Exploring Economic Significance of Forward Rate Spreads ===\")\n",
    "    \n",
    "    # Process data for SPX (as an example)\n",
    "    index_code = \"SPX\"\n",
    "    fut_file = Path(PROCESSED_DIR) / f\"{index_code}_Calendar_spread.csv\"\n",
    "    \n",
    "    if not fut_file.exists():\n",
    "        print(f\"Missing futures file for {index_code}. Cannot proceed with exploration.\")\n",
    "        return None\n",
    "    \n",
    "    # Run the forward rate calculation\n",
    "    df_res = process_index_forward_rates(index_code)\n",
    "    \n",
    "    if df_res is None or df_res.empty:\n",
    "        print(\"No valid results. Ending exploration.\")\n",
    "        return None\n",
    "    \n",
    "    # Set the spread column\n",
    "    spread_col = f\"spread_{index_code}\"\n",
    "    \n",
    "    \n",
    "    # Calculate the percentage of time the spread is positive vs negative\n",
    "    positive_spread = (df_res[spread_col] > 0).mean() * 100\n",
    "    negative_spread = (df_res[spread_col] < 0).mean() * 100\n",
    "    \n",
    "    print(f\"\\nSpread direction summary:\")\n",
    "    print(f\"  Positive spread: {positive_spread:.1f}% of the time\")\n",
    "    print(f\"  Negative spread: {negative_spread:.1f}% of the time\")\n",
    "    print(f\"  Zero or missing: {100 - positive_spread - negative_spread:.1f}% of the time\")\n",
    "    \n",
    "    # Visualize the distribution of spreads\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    sns.histplot(df_res[spread_col].dropna(), bins=50, kde=True)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.7)\n",
    "    plt.axvline(df_res[spread_col].mean(), color='r', linestyle='-', alpha=0.7, \n",
    "                label=f'Mean: {df_res[spread_col].mean():.2f} bps')\n",
    "    \n",
    "    plt.title(f\"{index_code} Forward Rate Spread Distribution (bps)\")\n",
    "    plt.xlabel(\"Spread (bps)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a time plot with positive/negative highlighting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot the spread\n",
    "    plt.plot(df_res.index, df_res[spread_col], 'k-', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Highlight positive and negative regions\n",
    "    positive_mask = df_res[spread_col] > 0\n",
    "    negative_mask = df_res[spread_col] < 0\n",
    "    \n",
    "    plt.fill_between(df_res.index, 0, df_res[spread_col], where=positive_mask, \n",
    "                     color='green', alpha=0.3, label='Positive Spread')\n",
    "    plt.fill_between(df_res.index, 0, df_res[spread_col], where=negative_mask, \n",
    "                     color='red', alpha=0.3, label='Negative Spread')\n",
    "    \n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.7)\n",
    "    plt.title(f\"{index_code} Forward Rate Spread Over Time (bps)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Spread (bps)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate persistence - how long spreads stay positive or negative\n",
    "    df_res['spread_sign'] = np.sign(df_res[spread_col])\n",
    "    df_res['regime_change'] = df_res['spread_sign'].diff().ne(0).astype(int)\n",
    "    df_res['regime_id'] = df_res['regime_change'].cumsum()\n",
    "    \n",
    "    # Get regime lengths\n",
    "    regime_lengths = df_res.groupby('regime_id').size()\n",
    "    \n",
    "    # Separate positive and negative regimes\n",
    "    regime_signs = df_res.groupby('regime_id')['spread_sign'].first()\n",
    "    \n",
    "    positive_lengths = regime_lengths[regime_signs > 0]\n",
    "    negative_lengths = regime_lengths[regime_signs < 0]\n",
    "    \n",
    "    print(\"\\nRegime persistence:\")\n",
    "    if not positive_lengths.empty:\n",
    "        print(f\"  Positive spread regimes: {len(positive_lengths)}\")\n",
    "        print(f\"  Average positive regime duration: {positive_lengths.mean():.1f} days\")\n",
    "        print(f\"  Longest positive regime: {positive_lengths.max()} days\")\n",
    "    \n",
    "    if not negative_lengths.empty:\n",
    "        print(f\"  Negative spread regimes: {len(negative_lengths)}\")\n",
    "        print(f\"  Average negative regime duration: {negative_lengths.mean():.1f} days\")\n",
    "        print(f\"  Longest negative regime: {negative_lengths.max()} days\")\n",
    "    \n",
    "    # Visualize regime persistence with a boxplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    data = [positive_lengths, negative_lengths]\n",
    "    labels = ['Positive Spread', 'Negative Spread']\n",
    "    \n",
    "    plt.boxplot(data, labels=labels, patch_artist=True, \n",
    "                boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                whiskerprops=dict(color='blue'),\n",
    "                capprops=dict(color='blue'),\n",
    "                medianprops=dict(color='red'))\n",
    "    \n",
    "    plt.title(f\"{index_code} Forward Rate Spread Regime Persistence\")\n",
    "    plt.ylabel(\"Duration (Days)\")\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return df_res\n",
    "\n",
    "# Run the exploration\n",
    "economic_df = explore_economic_significance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f3e66e-1640-42c6-bb5e-ccb3d6e63f5d",
   "metadata": {},
   "source": [
    "# Theoretical Understanding of Implied Forward Rate Calculation\n",
    "\n",
    "## 1. Understanding Daily Dividends\n",
    "\n",
    "The daily dividend processing is a crucial first step in forward rate calculation. It involves:\n",
    "\n",
    "- **Data Source**: Extracting dividend data from Bloomberg historical data\n",
    "- **Processing Steps**: Converting dates, handling missing values, and calculating cumulative dividends\n",
    "- **Key Insight**: Dividends accumulate over time and impact future values of the index\n",
    "- **Seasonality**: There are often patterns in dividend distributions throughout the year\n",
    "- **Importance**: Accurate dividend projections are essential for proper forward rate calculation\n",
    "\n",
    "Understanding how dividends accumulate between the current date and settlement dates is critical. The spread between nearby and farther-term futures contracts must account for these dividend payments.\n",
    "\n",
    "## 2. Barndorff-Nielsen Outlier Filter\n",
    "\n",
    "The Barndorff-Nielsen filter identifies and removes statistical outliers in time series data:\n",
    "\n",
    "- **Core Components**:\n",
    "  - Rolling median calculation (robust central tendency measure)\n",
    "  - Absolute deviation from the median\n",
    "  - Mean absolute deviation (MAD) as a scale measure\n",
    "  - Standardized deviation (absolute deviation / MAD)\n",
    "  - Threshold comparison to identify outliers\n",
    "\n",
    "- **Key Advantages**:\n",
    "  - Insensitive to extreme values (unlike mean/standard deviation)\n",
    "  - Adapts to local characteristics of the time series\n",
    "  - Preserves legitimate signal changes while removing anomalies\n",
    "\n",
    "- **Application to Forward Rates**:\n",
    "  - Removes data errors or market dislocations that would distort analysis\n",
    "  - Helps identify true arbitrage opportunities vs. data anomalies\n",
    "\n",
    "## 3. Forward Rate Calculation Process\n",
    "\n",
    "The forward rate calculation is based on the cost-of-carry model with dividends:\n",
    "\n",
    "- **Step 1: Data Merging**\n",
    "  - Calendar spread data (near/next futures prices and settlement dates)\n",
    "  - OIS rates (risk-free benchmark)\n",
    "  - Daily dividends (accumulation between current and settlement dates)\n",
    "\n",
    "- **Step 2: Dividend Sum Calculation**\n",
    "  - Calculate expected dividends between observation date and both settlement dates\n",
    "  - Apply half-interval compounding using the OIS rate to account for time value\n",
    "\n",
    "- **Step 3: Implied Forward Rate Calculation**\n",
    "  - Calculate raw implied forward rate from futures prices and compounded dividends\n",
    "  - Annualize the rate by dividing by the time difference and multiplying by 360\n",
    "\n",
    "- **Step 4: OIS-implied Forward Rate**\n",
    "  - Calculate the theoretical forward rate based solely on OIS rates\n",
    "  - This represents the risk-free expectation of future rates\n",
    "\n",
    "- **Step 5: Spread Calculation**\n",
    "  - The difference between implied and OIS forward rates represents potential arbitrage\n",
    "  - Expressed in basis points for easier interpretation\n",
    "\n",
    "The theoretical foundation is that under no-arbitrage conditions, the spread should be zero. Deviations represent market inefficiencies or risk premia.\n",
    "\n",
    "## 4. Cross-Index Analysis\n",
    "\n",
    "Comparing forward rate spreads across different indices offers deeper insights:\n",
    "\n",
    "- **Correlation Analysis**:\n",
    "  - High correlation suggests common market drivers\n",
    "  - Low correlation indicates index-specific factors\n",
    "\n",
    "- **Z-Score Normalization**:\n",
    "  - Allows comparison of spreads with different volatility characteristics\n",
    "  - Highlights periods of abnormal behavior across indices\n",
    "\n",
    "- **Rolling Correlation**:\n",
    "  - Shows how relationships between indices evolve over time\n",
    "  - Can identify regime changes in market integration\n",
    "\n",
    "Differences in spread behavior across indices may reveal:\n",
    "- Different dividend expectations\n",
    "- Index-specific liquidity constraints\n",
    "- Varying market microstructure\n",
    "\n",
    "\n",
    "## 5. Theoretical Framework\n",
    "\n",
    "The implied forward rate calculation is grounded in financial theory:\n",
    "\n",
    "- **Cost-of-Carry Model**:\n",
    "  - F(t,T) = S(t) × e^((r-d)(T-t))\n",
    "  - Where F is futures price, S is spot price, r is risk-free rate, d is dividend yield\n",
    "\n",
    "- **No-Arbitrage Principle**:\n",
    "  - In perfectly efficient markets with no frictions, the implied forward rate should equal the OIS forward rate\n",
    "  - Deviations create arbitrage opportunities\n",
    "\n",
    "- **Yield Curve Theory**:\n",
    "  - Forward rates contain information about market expectations of future interest rates\n",
    "  - Term structure shapes influence the relationship between near and far-term rates\n",
    "\n",
    "- **Funding Liquidity**:\n",
    "  - Constraints in funding can create persistent deviations from theoretical values\n",
    "  - The spread can be viewed as a measure of funding conditions\n",
    "\n",
    "- **Dividend Uncertainty**:\n",
    "  - Differences between expected and actual dividends affect the spread\n",
    "  - Market pricing may include a risk premium for dividend uncertainty\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
