{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2eaea6-0fea-4003-bff3-d9548860e875",
   "metadata": {},
   "source": [
    "# Futures Data Processing Walkthrough\n",
    "\n",
    "This notebook demonstrates the data processing pipeline for equity index futures data, which is a critical component of our equity spot-futures arbitrage analysis project.\n",
    "\n",
    "## Project Context: Equity Spot-Futures Arbitrage\n",
    "\n",
    "Our project analyzes arbitrage opportunities between equity spot and futures markets, focusing on three major indices:\n",
    "- S&P 500 (SPX with ES futures)\n",
    "- Nasdaq 100 (NDX with NQ futures)\n",
    "- Dow Jones Industrial Average (INDU with DM futures)\n",
    "\n",
    "To calculate implied forward rates for arbitrage analysis, we need properly processed futures data with accurate settlement dates and time-to-maturity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a686189-d45b-4d88-ba77-ecb171654caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the src directory to the path to import our settings\n",
    "sys.path.insert(1, \"./src\")\n",
    "try:\n",
    "    from settings import config\n",
    "    print(\"Successfully imported config from settings module\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import config. Make sure your working directory is set correctly.\")\n",
    "    def config(key):\n",
    "        config_dict = {\n",
    "            \"DATA_DIR\": Path(\"./_data\"),\n",
    "            \"TEMP_DIR\": Path(\"./_data/temp\"),\n",
    "            \"INPUT_DIR\": Path(\"./_data/input\"),\n",
    "            \"PROCESSED_DIR\": Path(\"./_data/processed\"),\n",
    "            \"MANUAL_DATA_DIR\": Path(\"./data_manual\"),\n",
    "            \"OUTPUT_DIR\":Path(\"./_output\"),\n",
    "        }\n",
    "        return config_dict.get(key, Path(\"./data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a380ca-3616-4eb9-9045-6f8adc3a4846",
   "metadata": {},
   "source": [
    "## Load Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf4593-990c-4be4-a5f1-707ca09abc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration paths\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "TEMP_DIR = config(\"TEMP_DIR\")\n",
    "INPUT_DIR = config(\"INPUT_DIR\")\n",
    "PROCESSED_DIR = config(\"PROCESSED_DIR\")\n",
    "DATA_MANUAL = config(\"MANUAL_DATA_DIR\")\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [DATA_DIR, TEMP_DIR, INPUT_DIR, PROCESSED_DIR, DATA_MANUAL]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Set up logging\n",
    "log_file = TEMP_DIR / 'futures_processing_notebook.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Logging configured successfully\")\n",
    "logger.info(f\"Log file will be saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ff480-2970-449b-a674-5f323cfac350",
   "metadata": {},
   "source": [
    "## Define Utility Functions\n",
    "We need several utility functions to properly process futures contract data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c9f16-0d9f-4782-92fd-9d2714db4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_third_friday(year, month):\n",
    "    \"\"\"\n",
    "    Calculate the third Friday of a given month and year.\n",
    "    \n",
    "    Args:\n",
    "        year (int): Year\n",
    "        month (int): Month (1-12)\n",
    "        \n",
    "    Returns:\n",
    "        datetime: Date object for the third Friday\n",
    "    \"\"\"\n",
    "    # Use calendar.monthcalendar: each week is a list of ints (0 if day not in month)\n",
    "    month_cal = calendar.monthcalendar(year, month)\n",
    "    # The first week that has a Friday (weekday index 4)\n",
    "    fridays = [week[calendar.FRIDAY] for week in month_cal if week[calendar.FRIDAY] != 0]\n",
    "    if len(fridays) < 3:\n",
    "        raise ValueError(f\"Not enough Fridays in {year}-{month}\")\n",
    "    return datetime(year, month, fridays[2])  # third Friday\n",
    "\n",
    "def parse_contract_month_year(contract_str):\n",
    "    \"\"\"\n",
    "    Parse Bloomberg's contract month/year string (e.g., 'DEC 10') into\n",
    "    a month number and a full year.\n",
    "    \n",
    "    Args:\n",
    "        contract_str (str): Contract month/year string\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (month_num, year_full) or (None, None) if invalid.\n",
    "    \"\"\"\n",
    "    if pd.isna(contract_str) or contract_str.strip() == '':\n",
    "        return None, None\n",
    "    parts = contract_str.split()\n",
    "    if len(parts) != 2:\n",
    "        logger.warning(f\"Unexpected contract format: {contract_str}\")\n",
    "        return None, None\n",
    "    month_abbr, year_abbr = parts\n",
    "    allowed = {\"MAR\": 3, \"JUN\": 6, \"SEP\": 9, \"DEC\": 12}\n",
    "    if month_abbr.upper() not in allowed:\n",
    "        raise ValueError(f\"Contract month {month_abbr} not in allowed set {list(allowed.keys())}\")\n",
    "    month_num = allowed[month_abbr.upper()]\n",
    "    try:\n",
    "        yr = int(year_abbr)\n",
    "        year_full = 2000 + yr if yr < 50 else 1900 + yr\n",
    "    except ValueError:\n",
    "        logger.warning(f\"Could not parse year: {year_abbr}\")\n",
    "        return None, None\n",
    "    return month_num, year_full\n",
    "\n",
    "# Test our utility functions with a couple of examples\n",
    "print(f\"Third Friday of March 2023: {get_third_friday(2023, 3)}\")\n",
    "print(f\"Parsing 'DEC 22': {parse_contract_month_year('DEC 22')}\")\n",
    "print(f\"Parsing 'MAR 23': {parse_contract_month_year('MAR 23')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be855bb9-3649-4153-9e12-29841eb04b21",
   "metadata": {},
   "source": [
    "## Load Bloomberg Data\n",
    "\n",
    "Let's load the Bloomberg futures data from the Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305ea08-0bed-404b-81df-08a7f8c5ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    INPUT_FILE = INPUT_DIR / \"bloomberg_historical_data.parquet\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        logger.warning(\"Primary input file not found, switching to cached data\")\n",
    "        INPUT_FILE = DATA_MANUAL / \"bloomberg_historical_data.parquet\"\n",
    "    \n",
    "    raw_data = pd.read_parquet(INPUT_FILE)\n",
    "    logger.info(f\"Successfully loaded data from {INPUT_FILE}\")\n",
    "    \n",
    "    # Convert index to DatetimeIndex if it's not already\n",
    "    if not isinstance(raw_data.index, pd.DatetimeIndex):\n",
    "        raw_data.index = pd.to_datetime(raw_data.index)\n",
    "        logger.info(\"Converted index to DatetimeIndex\")\n",
    "    \n",
    "    # Display the column structure (MultiIndex)\n",
    "    logger.info(f\"Column levels: {raw_data.columns.names}\")\n",
    "    print(\"\\nSample of MultiIndex columns:\")\n",
    "    for i, col in enumerate(raw_data.columns[:10]):\n",
    "        print(f\"  {col}\")\n",
    "    print(f\"  ...and {len(raw_data.columns)-10} more columns\")\n",
    "    \n",
    "    # Display basic info about the dataset\n",
    "    print(f\"\\nDataset shape: {raw_data.shape}\")\n",
    "    print(f\"Date range: {raw_data.index.min()} to {raw_data.index.max()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517f5b4-c7ab-49d2-b62f-16f8617513b6",
   "metadata": {},
   "source": [
    "## Define Index Futures Mapping\n",
    "\n",
    "Let's define the mapping between equity indices and their futures contracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65c79e-cd64-410a-b809-bad682378784",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {\n",
    "    'SPX': ['ES1', 'ES2', 'ES3', 'ES4'],  # S&P 500 futures\n",
    "    'NDX': ['NQ1', 'NQ2', 'NQ3', 'NQ4'],  # Nasdaq 100 futures\n",
    "    'INDU': ['DM1', 'DM2', 'DM3', 'DM4']  # Dow Jones futures\n",
    "}\n",
    "\n",
    "print(\"Equity indices and their futures contracts:\")\n",
    "for index, futures in indices.items():\n",
    "    print(f\"  {index}: {', '.join(futures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d3ac2-ee3b-4ef7-b553-12d2ac17aeb8",
   "metadata": {},
   "source": [
    "## Process Futures Data\n",
    "\n",
    "Now, let's implement the function that processes futures data for a single index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3557031-5ae8-4277-94ee-13490f918e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_index_futures(data, futures_codes):\n",
    "    \"\"\"\n",
    "    Process futures data for one index.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Multi-index DataFrame with Bloomberg data (indexed by Date)\n",
    "        futures_codes (list): List of futures codes (e.g., ['ES1', 'ES2', ...])\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of processed DataFrames, one for each futures code.\n",
    "              Each DataFrame contains:\n",
    "                  - Date\n",
    "                  - Futures_Price (from PX_LAST)\n",
    "                  - Volume, OpenInterest (if available)\n",
    "                  - ContractSpec (raw CURRENT_CONTRACT_MONTH_YR)\n",
    "                  - SettlementDate (actual settlement, 3rd Friday)\n",
    "                  - TTM (time-to-maturity in days)\n",
    "    \"\"\"\n",
    "    result_dfs = {}\n",
    "    for code in futures_codes:\n",
    "        logger.info(f\"Processing futures data for {code} Index\")\n",
    "        try:\n",
    "            # Extract columns for this contract\n",
    "            price_series = data.loc[:, (f'{code} Index', 'PX_LAST')]\n",
    "            volume_series = data.loc[:, (f'{code} Index', 'PX_VOLUME')]\n",
    "            oi_series = data.loc[:, (f'{code} Index', 'OPEN_INT')]\n",
    "            contract_series = data.loc[:, (f'{code} Index', 'CURRENT_CONTRACT_MONTH_YR')]\n",
    "            \n",
    "            # Create a DataFrame for this contract; index is Date (from raw data)\n",
    "            df_contract = pd.DataFrame({\n",
    "                'Date': data.index,\n",
    "                'Futures_Price': price_series,\n",
    "                'Volume': volume_series,\n",
    "                'OpenInterest': oi_series,\n",
    "                'ContractSpec': contract_series\n",
    "            })\n",
    "            df_contract = df_contract.reset_index(drop=True)\n",
    "            \n",
    "            # Parse contract specification and compute settlement date\n",
    "            settlement_dates = []\n",
    "            for cs in df_contract['ContractSpec']:\n",
    "                month_num, year_full = parse_contract_month_year(cs)\n",
    "                if month_num is None or year_full is None:\n",
    "                    settlement_dates.append(None)\n",
    "                else:\n",
    "                    settlement_dates.append(get_third_friday(year_full, month_num))\n",
    "            df_contract['SettlementDate'] = pd.to_datetime(settlement_dates)\n",
    "            \n",
    "            # Compute TTM in days: SettlementDate - Date\n",
    "            df_contract['Date'] = pd.to_datetime(df_contract['Date'])\n",
    "            df_contract['TTM'] = (df_contract['SettlementDate'] - df_contract['Date']).dt.days\n",
    "            \n",
    "            # Drop rows with missing TTM (if settlement date couldn't be computed)\n",
    "            df_contract = df_contract.dropna(subset=['TTM'])\n",
    "            result_dfs[code] = df_contract\n",
    "            logger.info(f\"Processed {code}: {len(df_contract)} rows\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {code}: {e}\")\n",
    "            continue\n",
    "    return result_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33e203-dfeb-4eeb-bc16-363ca608201f",
   "metadata": {},
   "source": [
    "## Process All Indices\n",
    "\n",
    "Let's apply our processing function to all indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780aca6-2f3f-4069-816a-6af2eed18e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_futures = {}\n",
    "for index_code, futures_codes in indices.items():\n",
    "    logger.info(f\"Processing futures for index {index_code}\")\n",
    "    processed = process_index_futures(raw_data, futures_codes)\n",
    "    all_futures[index_code] = processed\n",
    "    \n",
    "    # Display a sample of the processed data for each index\n",
    "    print(f\"\\nProcessed Futures for {index_code}:\")\n",
    "    for code, df in processed.items():\n",
    "        print(f\"  {code} - Shape: {df.shape}\")\n",
    "        display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f1f7d-c523-461f-81a7-0df6507401ac",
   "metadata": {},
   "source": [
    "## Analyze Contract Roll Patterns\n",
    "\n",
    "Let's visualize how contracts roll over time for one of the indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fb36d-582b-438d-8ed0-07b598e84e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose SPX ES1 (front-month) contract for visualization\n",
    "es1_df = all_futures['SPX']['ES1']\n",
    "\n",
    "# Plot settlement date changes over time to visualize contract rolls\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(es1_df['Date'], es1_df['SettlementDate'], 'b.-')\n",
    "plt.title('ES1 Contract Settlement Date Over Time (Contract Roll Pattern)', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Settlement Date', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the visualization\n",
    "plt.savefig(OUTPUT_DIR / 'es1_contract_roll_pattern.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Analyze TTM distribution for ES1\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(es1_df['TTM'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.axvline(es1_df['TTM'].mean(), color='red', linestyle='dashed', linewidth=1, label=f'Mean: {es1_df[\"TTM\"].mean():.1f} days')\n",
    "plt.title('Distribution of Time-to-Maturity (TTM) for ES1 Contract', fontsize=14)\n",
    "plt.xlabel('Days to Maturity', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the visualization\n",
    "plt.savefig(OUTPUT_DIR / 'es1_ttm_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de267837-556e-414b-bbd3-23fc064ad0cb",
   "metadata": {},
   "source": [
    "## Create Calendar Spreads\n",
    "\n",
    "Now let's implement the function to merge the Term 1 (nearest) and Term 2 (next nearest) contracts to create calendar spreads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93fe25d-6a5e-4b7d-93c4-e566b2d36e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_calendar_spreads(all_futures):\n",
    "    \"\"\"\n",
    "    For each index, merge the processed data for the two nearest futures contracts (Term 1 and Term 2)\n",
    "    on the Date field, and then combine the calendar spreads for all indices.\n",
    "    \n",
    "    Args:\n",
    "        all_futures (dict): Dictionary keyed by index code (e.g., 'SPX', 'NDX', 'INDU') where the value is\n",
    "                            another dictionary mapping futures code to its processed DataFrame.\n",
    "                            \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined calendar spread data for all indices.\n",
    "    \"\"\"\n",
    "    combined = []\n",
    "    # For each index, the first two codes are the two nearest contracts.\n",
    "    for index_code, fut_dict in all_futures.items():\n",
    "        # Identify term1 and term2 codes:\n",
    "        codes = list(fut_dict.keys())\n",
    "        if len(codes) < 2:\n",
    "            logger.warning(f\"Not enough futures data for {index_code}\")\n",
    "            continue\n",
    "        term1 = fut_dict[codes[0]].copy()\n",
    "        term2 = fut_dict[codes[1]].copy()\n",
    "        # Add a prefix so that we can merge and distinguish columns:\n",
    "        term1 = term1.add_prefix('Term1_')\n",
    "        term2 = term2.add_prefix('Term2_')\n",
    "        # Rename the Date columns back to 'Date' for merging\n",
    "        term1.rename(columns={'Term1_Date': 'Date'}, inplace=True)\n",
    "        term2.rename(columns={'Term2_Date': 'Date'}, inplace=True)\n",
    "        merged = pd.merge(term1, term2, on='Date', how='inner')\n",
    "        merged['Index'] = index_code\n",
    "        combined.append(merged)\n",
    "    if combined:\n",
    "        combined_df = pd.concat(combined, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        logger.warning(\"No valid calendar spread data to combine\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b634662-9f92-44f8-b017-79e96d05a862",
   "metadata": {},
   "source": [
    "## Create and Analyze Calendar Spreads\n",
    "\n",
    "Let's apply the function to create calendar spreads and analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438632d-c6e9-4f2b-a4d1-ac9b6570c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calendar spreads\n",
    "combined_spreads = merge_calendar_spreads(all_futures)\n",
    "\n",
    "# Display a sample of the calendar spread data\n",
    "print(\"Sample of Combined Calendar Spread Data:\")\n",
    "display(combined_spreads.head())\n",
    "\n",
    "# Check key statistics for calendar spreads\n",
    "print(\"\\nCalendar Spread Statistics by Index:\")\n",
    "for index in combined_spreads['Index'].unique():\n",
    "    index_df = combined_spreads[combined_spreads['Index'] == index]\n",
    "    print(f\"\\n{index} Index:\")\n",
    "    print(f\"  Number of observations: {len(index_df)}\")\n",
    "    print(f\"  Average Term1 TTM: {index_df['Term1_TTM'].mean():.2f} days\")\n",
    "    print(f\"  Average Term2 TTM: {index_df['Term2_TTM'].mean():.2f} days\")\n",
    "    print(f\"  Average calendar spread (TTM2 - TTM1): {(index_df['Term2_TTM'] - index_df['Term1_TTM']).mean():.2f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f27b63-07dc-4277-be94-44a3f47f66f2",
   "metadata": {},
   "source": [
    "It makes sense that the average calendar spread is over 3 months between contract 1 and contract 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009ae82b-d779-477b-9e12-ad9317f1c691",
   "metadata": {},
   "source": [
    "## Data Quality Checks\n",
    "\n",
    "Let's perform some data quality checks on our processed futures data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fef7d2-9193-4c7a-8139-e18ee34d1a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_futures_data_quality(combined_spreads):\n",
    "    \"\"\"\n",
    "    Perform data quality checks on the processed futures data.\n",
    "    \"\"\"\n",
    "    print(\"Running data quality checks on futures data...\")\n",
    "    tests_passed = 0\n",
    "    tests_failed = 0\n",
    "    \n",
    "    # Test 1: Check that we have data for all three indices\n",
    "    try:\n",
    "        expected_indices = {'SPX', 'NDX', 'INDU'}\n",
    "        actual_indices = set(combined_spreads['Index'].unique())\n",
    "        assert expected_indices == actual_indices, f\"Missing indices: {expected_indices - actual_indices}\"\n",
    "        print(f\"Test 1 Passed: All three indices are present\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 1 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Test 2: Check that Term2_TTM > Term1_TTM for all rows\n",
    "    try:\n",
    "        assert all(combined_spreads['Term2_TTM'] > combined_spreads['Term1_TTM']), \"Term2 TTM should always be greater than Term1 TTM\"\n",
    "        print(f\"Test 2 Passed: Term2 TTM is always greater than Term1 TTM\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 2 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "        \n",
    "    # Test 3: Check that all TTM values are reasonable (positive and typically under 365 days)\n",
    "    try:\n",
    "        assert all(combined_spreads['Term1_TTM'] > 0), \"Term1 TTM should be positive\"\n",
    "        assert all(combined_spreads['Term2_TTM'] > 0), \"Term2 TTM should be positive\"\n",
    "        assert combined_spreads['Term1_TTM'].max() < 365, \"Term1 TTM should typically be less than 365 days\"\n",
    "        assert combined_spreads['Term2_TTM'].max() < 730, \"Term2 TTM should typically be less than 730 days\"\n",
    "        print(f\"Test 3 Passed: TTM values are within reasonable ranges\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 3 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Test 4: Check for missing values in critical columns\n",
    "    critical_columns = ['Term1_Futures_Price', 'Term2_Futures_Price', 'Term1_TTM', 'Term2_TTM']\n",
    "    try:\n",
    "        for col in critical_columns:\n",
    "            assert combined_spreads[col].isna().sum() == 0, f\"Missing values in {col}\"\n",
    "        print(f\"Test 4 Passed: No missing values in critical columns\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 4 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Test 5: Check that futures prices are positive\n",
    "    try:\n",
    "        assert all(combined_spreads['Term1_Futures_Price'] > 0), \"Term1 futures prices should be positive\"\n",
    "        assert all(combined_spreads['Term2_Futures_Price'] > 0), \"Term2 futures prices should be positive\"\n",
    "        print(f\"Test 5 Passed: All futures prices are positive\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 5 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    print(f\"\\nData quality checks complete: {tests_passed} passed, {tests_failed} failed\")\n",
    "    return tests_passed, tests_failed\n",
    "\n",
    "# Run the data quality checks\n",
    "tests_passed, tests_failed = check_futures_data_quality(combined_spreads)\n",
    "def check_futures_data_quality(combined_spreads):\n",
    "    \"\"\"\n",
    "    Perform data quality checks on the processed futures data.\n",
    "    \"\"\"\n",
    "    print(\"Running data quality checks on futures data...\")\n",
    "    tests_passed = 0\n",
    "    tests_failed = 0\n",
    "    \n",
    "    # Test 1: Check that we have data for all three indices\n",
    "    try:\n",
    "        expected_indices = {'SPX', 'NDX', 'INDU'}\n",
    "        actual_indices = set(combined_spreads['Index'].unique())\n",
    "        assert expected_indices == actual_indices, f\"Missing indices: {expected_indices - actual_indices}\"\n",
    "        print(f\"Test 1 Passed: All three indices are present\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 1 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Test 2: Check that Term2_TTM > Term1_TTM for all rows\n",
    "    try:\n",
    "        assert all(combined_spreads['Term2_TTM'] > combined_spreads['Term1_TTM']), \"Term2 TTM should always be greater than Term1 TTM\"\n",
    "        print(f\"Test 2 Passed: Term2 TTM is always greater than Term1 TTM\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 2 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "        \n",
    "    # Test 3: Check that all TTM values are reasonable (positive and typically under 365 days)\n",
    "    try:\n",
    "        assert all(combined_spreads['Term1_TTM'] > 0), \"Term1 TTM should be positive\"\n",
    "        assert all(combined_spreads['Term2_TTM'] > 0), \"Term2 TTM should be positive\"\n",
    "        assert combined_spreads['Term1_TTM'].max() < 365, \"Term1 TTM should typically be less than 365 days\"\n",
    "        assert combined_spreads['Term2_TTM'].max() < 730, \"Term2 TTM should typically be less than 730 days\"\n",
    "        print(f\"Test 3 Passed: TTM values are within reasonable ranges\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 3 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Test 4: Check for missing values in critical columns\n",
    "    critical_columns = ['Term1_Futures_Price', 'Term2_Futures_Price', 'Term1_TTM', 'Term2_TTM']\n",
    "    try:\n",
    "        for col in critical_columns:\n",
    "            assert combined_spreads[col].isna().sum() == 0, f\"Missing values in {col}\"\n",
    "        print(f\"Test 4 Passed: No missing values in critical columns\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 4 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Test 5: Check that futures prices are positive\n",
    "    try:\n",
    "        assert all(combined_spreads['Term1_Futures_Price'] > 0), \"Term1 futures prices should be positive\"\n",
    "        assert all(combined_spreads['Term2_Futures_Price'] > 0), \"Term2 futures prices should be positive\"\n",
    "        print(f\"Test 5 Passed: All futures prices are positive\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test 5 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    print(f\"\\nData quality checks complete: {tests_passed} passed, {tests_failed} failed\")\n",
    "    return tests_passed, tests_failed\n",
    "\n",
    "# Run the data quality checks\n",
    "tests_passed, tests_failed = check_futures_data_quality(combined_spreads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa29adc-ab7d-4b42-941d-d0eec5fac188",
   "metadata": {},
   "source": [
    "Interestingly, we do see a failed test case where TTM for Term 1 was not positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc78c4-36ef-4426-a3a0-912e1a73007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where Term1_TTM is not positive\n",
    "problematic_rows = combined_spreads[combined_spreads['Term1_TTM'] <= 0]\n",
    "\n",
    "# Display the count of problematic rows\n",
    "print(f\"Found {len(problematic_rows)} rows with non-positive Term1_TTM values\")\n",
    "\n",
    "# Display the problematic rows\n",
    "if len(problematic_rows) > 0:\n",
    "    print(\"\\nRows with non-positive Term1_TTM values:\")\n",
    "    display(problematic_rows)\n",
    "    \n",
    "    # Group by index to see distribution of issues\n",
    "    print(\"\\nDistribution by index:\")\n",
    "    index_counts = problematic_rows['Index'].value_counts()\n",
    "    display(index_counts)\n",
    "    \n",
    "    # Look at the date distribution\n",
    "    print(\"\\nDate range of problematic rows:\")\n",
    "    print(f\"Earliest: {problematic_rows['Date'].min()}\")\n",
    "    print(f\"Latest: {problematic_rows['Date'].max()}\")\n",
    "    \n",
    "    # Check term 2 values for these rows\n",
    "    print(\"\\nTerm2_TTM statistics for these rows:\")\n",
    "    print(f\"Min: {problematic_rows['Term2_TTM'].min()}\")\n",
    "    print(f\"Max: {problematic_rows['Term2_TTM'].max()}\")\n",
    "    print(f\"Mean: {problematic_rows['Term2_TTM'].mean()}\")\n",
    "    \n",
    "    # See if we can identify a pattern\n",
    "    print(\"\\nSample of original contract specifications for these rows:\")\n",
    "    display(problematic_rows[['Date', 'Index', 'Term1_ContractSpec', 'Term1_SettlementDate', 'Term1_TTM']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0430003-c1eb-44f1-b715-b60231a64fb6",
   "metadata": {},
   "source": [
    "After Analysis, we note that there are rows in the dataset for SPX index where the TTM was 0, corresponding to some of the rollover dates. We will resolve this in the code dowstream by adding these code blocks when merging:\n",
    "\n",
    "dt = merged_df[ttm2] - merged_df[ttm1]\n",
    "merged_df[f\"cal_{index_code}_rf\"] = np.where(\n",
    "    dt > 0,\n",
    "    100.0 * merged_df[\"implied_forward_raw\"] * (360.0 / dt),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "merged_df[f\"ois_fwd_{index_code}\"] = np.where(\n",
    "    dt > 0,\n",
    "    merged_df[\"ois_fwd_raw\"] * (360.0 / dt) * 100.0,\n",
    "    np.nan\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32311f-e766-45d2-83ec-0effa3640a59",
   "metadata": {},
   "source": [
    "## Futures Price Visualization\n",
    "\n",
    "Let's visualize the futures prices for Term1 and Term2 for each index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72287d4-36fd-4296-9f46-9c378927152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a multi-panel figure\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
    "indices = sorted(combined_spreads['Index'].unique())\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    index_df = combined_spreads[combined_spreads['Index'] == index]\n",
    "    \n",
    "    # Plot Term1 and Term2 prices\n",
    "    axes[i].plot(index_df['Date'], index_df['Term1_Futures_Price'], 'b-', label='Term1 (Nearby)')\n",
    "    axes[i].plot(index_df['Date'], index_df['Term2_Futures_Price'], 'r-', alpha=0.7, label='Term2 (Deferred)')\n",
    "    \n",
    "    axes[i].set_title(f'{index} Futures Prices', fontsize=14)\n",
    "    axes[i].set_ylabel('Price', fontsize=12)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(loc='upper left')\n",
    "\n",
    "# Set common labels\n",
    "axes[2].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the visualization\n",
    "plt.savefig(OUTPUT_DIR / 'futures_prices_by_index.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b9294-d71d-499a-a8ad-76fa990b6cb1",
   "metadata": {},
   "source": [
    "## Integration with Forward Rate Calculations\n",
    "\n",
    "The processed futures data is now ready for integration with the OIS rates data to calculate implied forward rates and arbitrage spreads.\n",
    "\n",
    "The key outputs from this processing pipeline include:\n",
    "\n",
    "1. **Individual Index Calendar Spreads**: Saved as CSV files (`SPX_Calendar_spread.csv`, `NDX_Calendar_spread.csv`, `INDU_Calendar_spread.csv`) in the `PROCESSED_DIR`.\n",
    "\n",
    "2. **Combined Calendar Spreads**: Saved as `all_indices_calendar_spreads.csv` in the `PROCESSED_DIR`.\n",
    "\n",
    "These files contain the following critical information:\n",
    "- **Term1_Futures_Price** and **Term2_Futures_Price**: Prices for the nearby and deferred futures contracts\n",
    "- **Term1_SettlementDate** and **Term2_SettlementDate**: Settlement dates for each contract\n",
    "- **Term1_TTM** and **Term2_TTM**: Time-to-maturity in days\n",
    "\n",
    "In the next step, these files will be used along with the processed OIS rates to calculate:\n",
    "1. Futures-implied forward rates\n",
    "2. OIS-implied forward rates\n",
    "3. Equity spot-futures arbitrage spreads\n",
    "\n",
    "The accurate processing of settlement dates and TTM is crucial for properly aligning the futures contracts with OIS rates and accumulating dividends over the correct time periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65cc2a4-fb00-4b8f-b19f-794392218508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
