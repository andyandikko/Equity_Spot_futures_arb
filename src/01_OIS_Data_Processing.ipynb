{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7eafad-2cfa-472a-9638-30d8d2aace81",
   "metadata": {},
   "source": [
    "# OIS Rate Data Processing Walkthrough\n",
    "\n",
    "This notebook demonstrates the data processing pipeline for Overnight Indexed Swap (OIS) rate data, which is an essential component of our equity spot-futures arbitrage analysis project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6d0cc-9a90-41d4-8ea6-5bc2e9603b99",
   "metadata": {},
   "source": [
    "## Project Context: Equity Spot-Futures Arbitrage\n",
    "\n",
    "Our project replicates a key component of the arbitrage analysis from Hazelkorn et al. (2021), focusing on the **equity spot-futures arbitrage spread**. This spread measures potential arbitrage opportunities between equity spot and futures markets.\n",
    "\n",
    "In a perfectly efficient market, the no-arbitrage futures price follows:\n",
    "\n",
    "$$F_{t,\\tau} = S_t(1+r^f_{t,\\tau}) - E^Q_t[D_{t,\\tau}]$$\n",
    "\n",
    "Where:\n",
    "- $F_{t,\\tau}$ is the futures price at time $t$ for maturity $t+\\tau$\n",
    "- $S_t$ is the spot price at time $t$\n",
    "- $r^f_{t,\\tau}$ is the risk-free rate\n",
    "- $E^Q_t[D_{t,\\tau}]$ is the expected dividend yield under risk-neutral measure\n",
    "\n",
    "Due to the timing mismatch between equity spot markets (close at 4:00 PM EST) and futures markets (close at 4:15 PM EST), we focus on the **implied forward rate** derived from two futures contracts with different maturities $\\tau_1 < \\tau_2$:\n",
    "\n",
    "$$1 + f^{\\tau_1,\\tau_2}_t = \\frac{F_{t,\\tau_2} + E^Q_t[D_{t,\\tau_2}]}{F_{t,\\tau_1} + E^Q_t[D_{t,\\tau_1}]}$$\n",
    "\n",
    "The equity spot-futures arbitrage spread is defined as:\n",
    "\n",
    "$$ESF_t = f^{\\tau_1,\\tau_2}_t - OIS3M_t$$\n",
    "\n",
    "where $OIS3M_t$ is the 3-month OIS rate, serving as our benchmark risk-free rate.\n",
    "\n",
    "**In this notebook, we focus on the processing of the 3-month OIS rate data (OIS3M), which is a critical component for calculating the ESF spread.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16ed5a-a635-44ba-8b18-300dc6b4eb4f",
   "metadata": {},
   "source": [
    "## Loading libraries and directories setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4707e-3248-462f-8299-f973ddbe8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, \"./src\")\n",
    "from settings import config\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af19ce-935f-487b-971d-6da83ad3c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'src' in os.getcwd():\n",
    "    os.chdir(os.path.pardir)\n",
    "    print(os.getcwd())\n",
    "else:\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a93ce-6d37-4a73-a5a4-2e6d28935ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "TEMP_DIR = config(\"TEMP_DIR\")\n",
    "INPUT_DIR = config(\"INPUT_DIR\")\n",
    "PROCESSED_DIR = config(\"PROCESSED_DIR\")\n",
    "DATA_MANUAL = config(\"MANUAL_DATA_DIR\")\n",
    "START_DATE = config(\"START_DATE\")\n",
    "END_DATE = config(\"END_DATE\")\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e9c02-773b-4780-a9cc-b4c99ad6d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = TEMP_DIR / f'ois_processing.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a7951-3eaa-4885-81cd-d87b46e07a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required column mapping\n",
    "OIS_TENORS = {\n",
    "    \"OIS_3M\": \"USSOC CMPN Curncy\",   # 3 Month OIS Rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f665f-11e1-469e-866d-ba80672a6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = Path(INPUT_DIR) / \"bloomberg_historical_data.parquet\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    logger.warning(\"Primary input file not found, switching to cached data\")\n",
    "    INPUT_FILE = Path(DATA_MANUAL) / \"bloomberg_historical_data.parquet\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933a843-68fe-4f8d-adbc-ebd7af30a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Loading OIS data from {INPUT_FILE}\")\n",
    "\n",
    "try:\n",
    "    ois_df = pd.read_parquet(INPUT_FILE)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading parquet file: {e}\")\n",
    "    raise\n",
    "\n",
    "logger.info(f\"Column levels: {ois_df.columns.names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f18bc-782f-4967-b3fb-63a3e2eae774",
   "metadata": {},
   "source": [
    "## Data Inspection and Extraction\n",
    "\n",
    "Let's examine the structure of the OIS data and extract the 3-month OIS rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced1fae-b1e6-4b55-be63-2bfb5e7f4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required OIS column is present\n",
    "required_col = OIS_TENORS[\"OIS_3M\"]\n",
    "if (required_col, \"PX_LAST\") not in ois_df.columns:\n",
    "    raise ValueError(f\"Missing required OIS column: {required_col}\")\n",
    "\n",
    "# Select only the required 3-month OIS rate column\n",
    "logger.info(\"Extracting 3-month OIS rate column\")\n",
    "ois_df = ois_df.loc[:, [(required_col, \"PX_LAST\")]]\n",
    "ois_df.columns = [\"OIS_3M\"]  # Rename to a clean column name\n",
    "\n",
    "# Display the first few rows\n",
    "display(ois_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278292c-585c-4114-9438-af187dbbbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "ois_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068c01f-813b-4206-81c3-aba1c805f28c",
   "metadata": {},
   "source": [
    "First thing to note about the data pulled from bloomberg is that it is structured in a multiindex format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f5574-7127-43ef-b21c-0c43ed41d33e",
   "metadata": {},
   "source": [
    "## Data Cleaning and Transformation\n",
    "\n",
    "Now we'll clean the data and convert OIS rates from percentage to decimal format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781fd69-1921-4dc2-941f-6400d76ebb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before cleaning\n",
    "missing_before = ois_df[\"OIS_3M\"].isna().sum()\n",
    "logger.info(f\"Missing values before cleaning: {missing_before}\")\n",
    "\n",
    "# Convert OIS rates from percentage to decimal format\n",
    "logger.info(\"Converting OIS_3M from percentage to decimal format\")\n",
    "ois_df[\"OIS_3M\"] = ois_df[\"OIS_3M\"] / 100\n",
    "\n",
    "# Display the first few rows after conversion\n",
    "display(ois_df.head())\n",
    "\n",
    "# Drop rows with missing values\n",
    "ois_df_clean = ois_df.dropna(subset=[\"OIS_3M\"])\n",
    "dropped_rows = len(ois_df) - len(ois_df_clean)\n",
    "logger.info(f\"Dropped {dropped_rows} rows with missing values\")\n",
    "ois_df = ois_df_clean\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n========== OIS Data Summary ==========\")\n",
    "print(f\"Shape of dataset: {ois_df.shape} (rows, columns)\")\n",
    "print(f\"Missing values after cleaning: {ois_df.isna().sum().iloc[0]}\")\n",
    "display(ois_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb590f27-9fc8-437a-af95-1a8e54baf0a7",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's visualize the 3-month OIS rate over time to understand its behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5bee6a-f4e9-40af-84af-3da8c2e7478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the index is a datetime for proper time series visualization\n",
    "if not isinstance(ois_df.index, pd.DatetimeIndex):\n",
    "    logger.warning(\"Converting index to datetime\")\n",
    "    ois_df.index = pd.to_datetime(ois_df.index)\n",
    "\n",
    "# Plot the 3-month OIS rate over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ois_df.index, ois_df['OIS_3M'], 'b-', linewidth=1.5)\n",
    "plt.title('3-Month OIS Rate (Decimal Format)', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Rate', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Calculate and display statistics\n",
    "mean_rate = ois_df['OIS_3M'].mean()\n",
    "std_rate = ois_df['OIS_3M'].std()\n",
    "min_rate = ois_df['OIS_3M'].min()\n",
    "max_rate = ois_df['OIS_3M'].max()\n",
    "\n",
    "plt.axhline(y=mean_rate, color='r', linestyle='--', alpha=0.7, label=f'Mean: {mean_rate:.4f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / 'ois_3m_rate_time_series.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\n3-Month OIS Rate Summary Statistics:\")\n",
    "print(f\"Mean: {mean_rate:.6f}\")\n",
    "print(f\"Standard Deviation: {std_rate:.6f}\")\n",
    "print(f\"Minimum: {min_rate:.6f}\")\n",
    "print(f\"Maximum: {max_rate:.6f}\")\n",
    "print(f\"Number of observations: {len(ois_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c163a-e8a9-413b-bf70-8598a07c878a",
   "metadata": {},
   "source": [
    "## Rolling Statistics Analysis\n",
    "\n",
    "Let's examine the volatility and trends in the OIS rate over different time windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea29b7b-76d5-4de8-9954-4b602c1d335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 30-day and 90-day rolling means and standard deviations\n",
    "rolling_mean_30d = ois_df['OIS_3M'].rolling(window=30).mean()\n",
    "rolling_std_30d = ois_df['OIS_3M'].rolling(window=30).std()\n",
    "rolling_mean_90d = ois_df['OIS_3M'].rolling(window=90).mean()\n",
    "rolling_std_90d = ois_df['OIS_3M'].rolling(window=90).std()\n",
    "\n",
    "# Plot rolling statistics\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot rolling means\n",
    "axs[0].plot(ois_df.index, ois_df['OIS_3M'], 'b-', alpha=0.5, label='OIS 3M Rate')\n",
    "axs[0].plot(ois_df.index, rolling_mean_30d, 'r-', label='30-day Rolling Mean')\n",
    "axs[0].plot(ois_df.index, rolling_mean_90d, 'g-', label='90-day Rolling Mean')\n",
    "axs[0].set_title('OIS 3M Rate with Rolling Means', fontsize=14)\n",
    "axs[0].set_ylabel('Rate', fontsize=12)\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot rolling standard deviations\n",
    "axs[1].plot(ois_df.index, rolling_std_30d, 'r-', label='30-day Rolling Volatility')\n",
    "axs[1].plot(ois_df.index, rolling_std_90d, 'g-', label='90-day Rolling Volatility')\n",
    "axs[1].set_title('OIS 3M Rate Volatility', fontsize=14)\n",
    "axs[1].set_xlabel('Date', fontsize=12)\n",
    "axs[1].set_ylabel('Standard Deviation', fontsize=12)\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'ois_3m_rolling_statistics.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Log the results\n",
    "logger.info(\"Generated rolling statistics and visualizations for the OIS 3M rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d6304-9cb8-4226-bf33-fb07216c8742",
   "metadata": {},
   "source": [
    "We can visually see that OIS rate has a time varying mean and time varying volatility component. Periods of rate volatility often correspond to wider arbitrage spreads as market participants face uncertainty about funding costs. Thus, visualing the patterns in OIS rates are essential is understanding spreads.\n",
    "\n",
    "\n",
    "**Post-Financial Crisis Period (2010-2015)**:\n",
    "\n",
    "Low and stable OIS rates (near zero) reflecting the Federal Reserve's accommodative monetary policy following the 2008-2009 financial crisis. Limited volatility as the Fed maintained its zero interest rate policy (ZIRP)\n",
    "\n",
    "\n",
    "**Gradual Normalization (2016-2019)**:\n",
    "\n",
    "Steady increase in OIS rates as the Federal Reserve implemented its policy normalization program. December 2015 marked the first rate hike after nearly a decade at zero. Reduced balance sheet and rising federal funds rate target led to a gradual upward trend in OIS rates\n",
    "\n",
    "\n",
    "**COVID-19 Pandemic Shock (2020)**:\n",
    "\n",
    "Sharp spike in volatility in March 2020 corresponding to the global COVID-19 market panic. Dramatic rate cuts by the Federal Reserve returning to near-zero rates. Implementation of extensive quantitative easing and emergency liquidity facilities. This period shows the highest short-term volatility in our dataset\n",
    "\n",
    "\n",
    "**Inflation Surge and Monetary Tightening (2022-2023)**:\n",
    "\n",
    "Pronounced upward trend in OIS rates beginning in early 2022. Reflects the Federal Reserve's aggressive rate hiking cycle in response to persistent inflation.OIS rates reached their highest levels in over a decade\n",
    "\n",
    "**Increasing volatility in rates, but falling levels (2024-2025)**:\n",
    "The Federal Open Market Committee (FOMC) cut the range by 50 basis points to 4.75 to 5.00 percent in mid-September and by another 25 basis points to 4.50 to 4.75 percent in early November. As expected, yields on short-term Treasury bills fell in kind, with the 3-month yield dropping from 4.87 percent on September 16 to 4.54 percent. [CRFB](https://www.crfb.org/blogs/fed-cuts-rates-treasury-yields-are-rising)\n",
    "\n",
    "The election day volatility surge could reflect a belief that the election outcome will lead to stronger economic growth, more inflation, additional debt, or some combination of the three. Though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a41c0-aff4-48ee-8afe-febe7bb0c8ca",
   "metadata": {},
   "source": [
    "## Unit Tests for OIS Data Processing\n",
    "\n",
    "Here we perform some sanity checking of the data processing steps performed here, which will be critical prior to the calculation of spreads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1b7e9-d334-412c-a67e-f2b8b6d0a117",
   "metadata": {},
   "source": [
    "Each unit test in our OIS data processing pipeline serves a specific purpose to ensure data quality and compatibility with downstream analysis. Below is the rationale behind each test:\n",
    "\n",
    "### Test 1: Check for Non-Empty DataFrame\n",
    "**Rationale:** An empty DataFrame would cause all subsequent calculations to fail. This basic check ensures we have data to work with before proceeding.\n",
    "\n",
    "### Test 2: Verify OIS_3M Column Exists\n",
    "**Rationale:** The entire analysis depends on the 3-month OIS rate. If this column is missing or incorrectly named, all downstream calculations would fail or produce meaningless results.\n",
    "\n",
    "### Test 3: Confirm Decimal Format Conversion\n",
    "**Rationale:** The downstream forward rate calculations require OIS rates in decimal format (e.g., 0.0325 rather than 3.25%). Using percentage values would amplify the implied forward rates by 100x, creating dramatically incorrect arbitrage spreads.\n",
    "\n",
    "### Test 4: Ensure No Missing Values\n",
    "**Rationale:** Missing OIS values would create gaps in our arbitrage spread calculations. We enforce a clean dataset here to prevent unexplained NaN values later in the analysis pipeline.\n",
    "\n",
    "### Test 5: Validate Reasonable Rate Range\n",
    "**Rationale:** OIS rates should be positive (>=0) and realistically below 50% (<0.5 in decimal). Values outside this range likely indicate data errors that would distort our arbitrage analysis.\n",
    "\n",
    "### Test 6: Verify DatetimeIndex\n",
    "**Rationale:** The as-of merge in the forward rate calculation requires a DatetimeIndex. This merge operation finds the latest OIS rate available on or before each futures observation date, making proper datetime indexing essential.\n",
    "\n",
    "### Test 7: Check Data Continuity for merging consistency (10-day threshold)\n",
    "**Rationale:** The as-of merge requires reasonable continuity in OIS data. We use a 10-day threshold because:\n",
    "1. Trading days typically occur 5 days per week\n",
    "2. A 10-day gap represents approximately two weeks of calendar time\n",
    "3. Longer gaps would mean using significantly outdated OIS rates in the merge\n",
    "4. The downstream equity arbitrage calculation uses OIS as a benchmark risk-free rate, which becomes unreliable if too stale\n",
    "5. Since we're using a backward-looking merge, large gaps in OIS data could mean many futures data points would match to the same outdated OIS rate.\n",
    "\n",
    "### Test 8: Confirm Sorted Index\n",
    "**Rationale:** The `pd.merge_asof()` function specifically requires sorted time series data. If the index isn't monotonically increasing, the as-of merge will fail or produce incorrect matches.\n",
    "\n",
    "### Test 9: Verify Date Range Coverage\n",
    "**Rationale:** We target at least 70% coverage of the specified date range because:\n",
    "1. Insufficient coverage could bias our arbitrage spread analysis toward certain time periods\n",
    "2. Most financial analyses require reasonable coverage across market cycles\n",
    "3. The coverage ratio accounts for weekends and holidays (when markets are closed)\n",
    "4. Significant gaps could lead to seasonal biases in our arbitrage spread analysis\n",
    "\n",
    "### Test 10: Validate CSV Output Format\n",
    "**Rationale:** The forward rate calculation script explicitly loads \"cleaned_ois_rates.csv\" and expects:\n",
    "1. A date column (either as 'Date' or as an index in 'Unnamed: 0')\n",
    "2. An 'OIS_3M' column with the decimal-formatted rates\n",
    "\n",
    "This test verifies that our output file meets these requirements, ensuring seamless integration with downstream processes.\n",
    "\n",
    "### As-Of Merge in Downstream Process\n",
    "The downstream script (`implied_forward_rate.py`) uses the as-of merge to pair OIS rates with futures data. A “backward” search selects the last row in the right DataFrame whose ‘on’ key is less than or equal to the left’s key. This means we effectively try to prevent data leakage by trying to use OIS rates on or prior to the other data date. Despite this we believe that most of the dates will be consistent:\n",
    "\n",
    "\n",
    "merged_df = pd.merge_asof(\n",
    "   fut_df, ois_df, on=\"Date\", direction=\"backward\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a204db-b025-4fad-bbcc-11623b2825b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ois_data(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts, cleans, and formats only the 3-month OIS rate from Bloomberg historical dataset.\n",
    "\n",
    "    Args:\n",
    "        filepath (Path): Path to the parquet file containing multi-index Bloomberg data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned OIS dataset containing only the 3-month OIS rate.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the required OIS column is missing.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading OIS data from {filepath}\")\n",
    "\n",
    "    try:\n",
    "        ois_df = pd.read_parquet(filepath)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading parquet file: {e}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(f\"Column levels: {ois_df.columns.names}\")\n",
    "\n",
    "    # Ensure required OIS column is present\n",
    "    required_col = OIS_TENORS[\"OIS_3M\"]\n",
    "    if (required_col, \"PX_LAST\") not in ois_df.columns:\n",
    "        raise ValueError(f\"Missing required OIS column: {required_col}\")\n",
    "\n",
    "    # Select only the required 3-month OIS rate column\n",
    "    ois_df = ois_df.loc[:, [(required_col, \"PX_LAST\")]]\n",
    "    ois_df.columns = [\"OIS_3M\"]  # Rename to a clean column name\n",
    "\n",
    "    # Convert OIS rates from percentage to decimal format (if applicable)\n",
    "    logger.info(\"Converting OIS_3M from percentage to decimal format\")\n",
    "    ois_df[\"OIS_3M\"] = ois_df[\"OIS_3M\"] / 100\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    ois_df = ois_df.dropna(subset=[\"OIS_3M\"])\n",
    "\n",
    "\n",
    "    # Log dataset summary\n",
    "    logger.info(\"\\n========== OIS Data Summary ==========\")\n",
    "    logger.info(f\"Shape of dataset: {ois_df.shape} (rows, columns)\")\n",
    "    logger.info(f\"Missing values per column:\\n{ois_df.isna().sum().to_string()}\")\n",
    "    logger.info(\"Descriptive statistics:\\n%s\", ois_df.describe().to_string())\n",
    "    logger.info(\"First 5 rows of cleaned OIS data:\\n%s\", ois_df.head().to_string())\n",
    "\n",
    "    return ois_df\n",
    "\n",
    "def test_ois_data_processing(ois_df):\n",
    "    \"\"\"Run unit tests for the OIS data processing function.\"\"\"\n",
    "    logger.info(\"Running unit tests for OIS data processing...\")\n",
    "    tests_passed = 0\n",
    "    tests_failed = 0\n",
    "    \n",
    "    # Test 1: Check that dataframe is not empty\n",
    "    try:\n",
    "        assert not ois_df.empty, \"OIS dataframe should not be empty\"\n",
    "        logger.info(\"Test 1 Passed: OIS dataframe is not empty\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        logger.error(f\"Test 1 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Test 2: Check that OIS_3M column exists\n",
    "    try:\n",
    "        assert \"OIS_3M\" in ois_df.columns, \"OIS_3M column should exist in the dataframe\"\n",
    "        logger.info(\"Test 2 Passed: OIS_3M column exists\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        logger.error(f\"Test 2 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    if \"OIS_3M\" in ois_df.columns and not ois_df.empty:\n",
    "        # Test 3: Check that OIS rates are converted to decimal format\n",
    "        try:\n",
    "            assert all(ois_df['OIS_3M'] < 1.0), \"OIS rates should be in decimal format (not percentage)\"\n",
    "            logger.info(\"Test 3 Passed: OIS rates are in decimal format\")\n",
    "            tests_passed += 1\n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"Test 3 Failed: {e}\")\n",
    "            tests_failed += 1\n",
    "        \n",
    "        # Test 4: Check that there are no missing values\n",
    "        try:\n",
    "            assert ois_df['OIS_3M'].isna().sum() == 0, \"There should be no missing values in the cleaned data\"\n",
    "            logger.info(\"Test 4 Passed: No missing values in cleaned data\")\n",
    "            tests_passed += 1\n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"Test 4 Failed: {e}\")\n",
    "            tests_failed += 1\n",
    "        \n",
    "        # Test 5: Check that the OIS rates are within a reasonable range for interest rates\n",
    "        try:\n",
    "            assert all(ois_df['OIS_3M'] >= 0), \"OIS rates should not be negative\"\n",
    "            assert all(ois_df['OIS_3M'] < 0.5), \"OIS rates should be less than 50% (0.5 in decimal)\"\n",
    "            logger.info(\"Test 5 Passed: OIS rates are within reasonable bounds\")\n",
    "            tests_passed += 1\n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"Test 5 Failed: {e}\")\n",
    "            tests_failed += 1\n",
    "    \n",
    "    # Test 6: Check that the index is a DatetimeIndex for proper as-of merging\n",
    "    try:\n",
    "        assert isinstance(ois_df.index, pd.DatetimeIndex), \"Index should be a DatetimeIndex\"\n",
    "        logger.info(\"Test 6 Passed: Index is a DatetimeIndex\")\n",
    "        tests_passed += 1\n",
    "    except AssertionError as e:\n",
    "        logger.error(f\"Test 6 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    # Only continue datetime-specific tests if index is proper\n",
    "    if isinstance(ois_df.index, pd.DatetimeIndex):\n",
    "        # Test 7: Check for data continuity (important for as-of merging)\n",
    "        try:\n",
    "            # Check if there are no large gaps in the data\n",
    "            date_diffs = ois_df.index.to_series().diff().dt.days.dropna()\n",
    "            \n",
    "            if not date_diffs.empty:\n",
    "                max_gap = date_diffs.max()\n",
    "                gap_days_threshold = 10  # Adjust as needed\n",
    "                assert max_gap <= gap_days_threshold, f\"Max gap in OIS data is {max_gap} days, should be ≤ {gap_days_threshold} days\"\n",
    "                logger.info(f\"Test 7 Passed: Maximum gap in OIS data is {max_gap} days\")\n",
    "                tests_passed += 1\n",
    "            else:\n",
    "                logger.warning(\"Test 7 Skipped: Insufficient data to check for gaps\")\n",
    "                tests_passed += 0.5  # Partial credit\n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"Test 7 Failed: {e}\")\n",
    "            tests_failed += 1\n",
    "        \n",
    "        # Test 8: Check for sorted index (required for as-of merging)\n",
    "        try:\n",
    "            assert ois_df.index.is_monotonic_increasing, \"Index must be sorted in ascending order\"\n",
    "            logger.info(\"Test 8 Passed: Index is properly sorted\")\n",
    "            tests_passed += 1\n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"Test 8 Failed: {e}\")\n",
    "            tests_failed += 1\n",
    "        \n",
    "        # Test 9: Check if index covers required date range\n",
    "        try:\n",
    "            start_date = pd.to_datetime(START_DATE)\n",
    "            end_date = pd.to_datetime(END_DATE)\n",
    "            \n",
    "            data_start = ois_df.index.min()\n",
    "            data_end = ois_df.index.max()\n",
    "            \n",
    "            coverage_ratio = len(ois_df) / (end_date - start_date).days\n",
    "            assert coverage_ratio >= 0.7, f\"Date coverage ratio is {coverage_ratio:.2f}, should be ≥ 0.7\"\n",
    "            \n",
    "            logger.info(f\"Test 9 Passed: Data covers from {data_start} to {data_end} with {coverage_ratio:.2f} coverage ratio\")\n",
    "            tests_passed += 1\n",
    "        except (ImportError, AssertionError) as e:\n",
    "            logger.error(f\"Test 9 Failed: {e}\")\n",
    "            tests_failed += 1\n",
    "    \n",
    "    # Test 10: Verify CSV output is correctly formatted\n",
    "    try:\n",
    "        output_path = Path(PROCESSED_DIR) / \"cleaned_ois_rates.csv\"\n",
    "        assert output_path.exists(), \"Output CSV file must exist\"\n",
    "        test_df = pd.read_csv(output_path)\n",
    "        \n",
    "        # Check that date column exists (either as 'Date' or index)\n",
    "        date_col_exists = any(col in [\"Date\", \"Unnamed: 0\"] for col in test_df.columns)\n",
    "        assert date_col_exists, \"CSV must have a date column for merging\"\n",
    "        \n",
    "        # Check that OIS column exists\n",
    "        assert \"OIS_3M\" in test_df.columns, \"CSV must have OIS_3M column\"\n",
    "        \n",
    "        logger.info(\"Test 10 Passed: CSV output is correctly formatted\")\n",
    "        tests_passed += 1\n",
    "    except (AssertionError, Exception) as e:\n",
    "        logger.error(f\"Test 10 Failed: {e}\")\n",
    "        tests_failed += 1\n",
    "    \n",
    "    logger.info(f\"Unit tests complete: {tests_passed} passed, {tests_failed} failed\")\n",
    "    return tests_passed, tests_failed\n",
    "\n",
    "try:\n",
    "    ois_df = process_ois_data(INPUT_FILE)\n",
    "    tests_passed, tests_failed = test_ois_data_processing(ois_df)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in processing or testing OIS data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77736e1a-4376-4bc9-a52b-aee863f110cb",
   "metadata": {},
   "source": [
    "## Saving summary table for latex document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8f57f-1a92-420d-b8e6-0022dbb9418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed OIS data\n",
    "ois_file = PROCESSED_DIR / \"cleaned_ois_rates.csv\"\n",
    "ois_df = pd.read_csv(ois_file)\n",
    "\n",
    "# Ensure date column is properly formatted\n",
    "if \"Date\" not in ois_df.columns and \"Unnamed: 0\" in ois_df.columns:\n",
    "    ois_df = ois_df.rename(columns={\"Unnamed: 0\": \"Date\"})\n",
    "ois_df[\"Date\"] = pd.to_datetime(ois_df[\"Date\"])\n",
    "ois_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Convert decimal rates back to percentage for better readability in the table\n",
    "ois_df[\"OIS_3M_pct\"] = ois_df[\"OIS_3M\"] * 100\n",
    "\n",
    "# Define monetary policy regimes\n",
    "regime_conditions = [\n",
    "    (ois_df.index < pd.Timestamp(\"2016-01-01\")),\n",
    "    (ois_df.index >= pd.Timestamp(\"2016-01-01\")) & (ois_df.index < pd.Timestamp(\"2020-01-01\")),\n",
    "    (ois_df.index >= pd.Timestamp(\"2020-01-01\")) & (ois_df.index < pd.Timestamp(\"2021-01-01\")),\n",
    "    (ois_df.index >= pd.Timestamp(\"2021-01-01\")) & (ois_df.index < pd.Timestamp(\"2024-01-01\")),\n",
    "    (ois_df.index >= pd.Timestamp(\"2024-01-01\"))\n",
    "]\n",
    "\n",
    "regime_names = [\n",
    "    \"ZIRP (2010-2015)\",\n",
    "    \"Normalization (2016-2019)\",\n",
    "    \"COVID-19 Shock (2020)\",\n",
    "    \"Inflation Surge (2021-2023)\",\n",
    "    \"Stabilization (2024)\"\n",
    "]\n",
    "\n",
    "ois_df[\"Regime\"] = np.select(regime_conditions, regime_names, default=\"Unknown\")\n",
    "\n",
    "# Calculate statistics by regime\n",
    "stats_by_regime = ois_df.groupby(\"Regime\")[\"OIS_3M_pct\"].agg(\n",
    "    [\"mean\", \"std\", \"min\", \"max\"]\n",
    ").reset_index()\n",
    "\n",
    "# Add full sample statistics\n",
    "full_sample_stats = pd.DataFrame({\n",
    "    \"Regime\": [\"Full Sample\"],\n",
    "    \"mean\": [ois_df[\"OIS_3M_pct\"].mean()],\n",
    "    \"std\": [ois_df[\"OIS_3M_pct\"].std()],\n",
    "    \"min\": [ois_df[\"OIS_3M_pct\"].min()],\n",
    "    \"max\": [ois_df[\"OIS_3M_pct\"].max()]\n",
    "})\n",
    "\n",
    "stats_by_regime = pd.concat([stats_by_regime, full_sample_stats])\n",
    "\n",
    "# Create the LaTeX table manually rather than using pandas to_latex\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Summary Statistics for 3-Month OIS Rate by Monetary Policy Regime. This table presents key statistical properties of the OIS rate across different monetary policy periods. The stark contrasts in mean levels and volatility across regimes highlight how funding conditions for arbitrageurs have varied dramatically over time. The low volatility during ZIRP (2010-2015) created stable funding conditions, while the high volatility during the COVID-19 shock and subsequent inflation surge created uncertainty that likely widened arbitrage spreads.}\n",
    "\\label{tab:ois_stats}\n",
    "\\begin{tabular}{lrrrr}\n",
    "\\toprule\n",
    "\\textbf{Period} & \\textbf{Mean (\\%)} & \\textbf{Std Dev (\\%)} & \\textbf{Min (\\%)} & \\textbf{Max (\\%)} \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Add table rows\n",
    "for _, row in stats_by_regime.iterrows():\n",
    "    regime = row['Regime']\n",
    "    # Bold the Full Sample row\n",
    "    if regime == \"Full Sample\":\n",
    "        regime = r\"\\textbf{Full Sample}\"\n",
    "        \n",
    "    latex_table += f\"{regime} & {row['mean']:.2f} & {row['std']:.2f} & {row['min']:.2f} & {row['max']:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "# Add the table footer\n",
    "latex_table += r\"\\bottomrule\" + \"\\n\"\n",
    "latex_table += r\"\\end{tabular}\" + \"\\n\"\n",
    "latex_table += r\"\\end{table}\"\n",
    "\n",
    "# Save the LaTeX table\n",
    "with open(OUTPUT_DIR / \"ois_summary_statistics.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"OIS summary statistics table saved to {OUTPUT_DIR / 'ois_summary_statistics.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcd625-d6a5-4580-96d7-c64615d2a3f1",
   "metadata": {},
   "source": [
    "## Integration with Equity Spot-Futures Arbitrage Analysis\n",
    "\n",
    "Our processed OIS data (stored in `cleaned_ois_rates.csv`) serves as a critical input for the next step in our equity spot-futures arbitrage analysis. Here's how our OIS data processing connects to the broader project:\n",
    "\n",
    "### How OIS Data Will Be Used\n",
    "\n",
    "1. **Input for Forward Rate Calculations**: \n",
    "   - The subsequent script (`implied_forward_rate.py`) will load our cleaned OIS rates and merge them with futures data\n",
    "   - The OIS rate serves as the benchmark risk-free rate in all calculations\n",
    "\n",
    "2. **Calculating Arbitrage Spreads**:\n",
    "   - The OIS rate will be used to compound dividends\n",
    "   - It will help calculate the theoretical OIS-implied forward rate\n",
    "   - The equity spot-futures arbitrage spread (ESF) will be calculated as the difference between:\n",
    "     * The futures-implied forward rate (derived from futures prices and dividend data)\n",
    "     * The OIS-implied forward rate (derived from our processed OIS data)\n",
    "\n",
    "3. **Multi-Index Analysis**:\n",
    "   - This process will be repeated for three major indices: S&P 500 (SPX), Nasdaq 100 (NDX), and Dow Jones (INDU)\n",
    "   - The spreads will be analyzed across these indices to identify market inefficiencies\n",
    "\n",
    "### Expected Outputs\n",
    "\n",
    "Once our OIS data is integrated with the futures data, we'll generate:\n",
    "\n",
    "1. Forward rate calculations for each index\n",
    "2. Filtered arbitrage spread series (with outliers removed)\n",
    "3. Visualizations comparing spreads across indices\n",
    "4. Analysis of spread behavior under different market conditions\n",
    "\n",
    "By properly processing the OIS data now, we ensure accurate measurement of funding cost differentials in the next phase of our equity spot-futures arbitrage project."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
